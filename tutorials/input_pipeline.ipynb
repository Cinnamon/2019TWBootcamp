{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yUUhv9uuIuBk",
    "outputId": "4c2c94cd-0aba-45c4-aa75-f4d94f1d3247"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vXeY_Y_eati"
   },
   "source": [
    "If you are using Google Colab, go to data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "g5OxH2yrLa-2",
    "outputId": "ec56071e-8062-40ed-ddf1-a4411b8cbdbc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/2019TWBootcamp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YupbhCORIuBo"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AY7_h0OmIuBo",
    "outputId": "b8038ac7-b0fa-4ee3-8e9c-a4067da9527f"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "#img_path = \"/home/Data/CharactersTrimPad28/\"\n",
    "#img_path = \"./s3mnt/ChineseNumbers/\"\n",
    "img_path = \"/home/Data/ChineseNumbers/\"\n",
    "data_root = pathlib.Path(img_path)\n",
    "if not data_root.exists():\n",
    "    print(\"{} not exist!\".format(data_root))\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 256 \n",
    "IMG_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k95tBs9gIuBq"
   },
   "source": [
    "## Generate filename and label list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ISBR3XBpIuBr",
    "outputId": "08b9115f-dcfc-4c3e-cf8f-b55a85505fcc"
   },
   "outputs": [],
   "source": [
    "# tf.data.Dataset.from_tensor_slices\n",
    "all_image_paths = [str(path) for path in list(data_root.glob('*/*'))]\n",
    "\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "image_count = len(all_image_paths) * NUM_EPOCHS\n",
    "\n",
    "for i in range(3):\n",
    "    print(all_image_paths[i])\n",
    "image_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgRBtZvWIuBs"
   },
   "source": [
    "## Mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVepb_l3IuBt"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_str):\n",
    "    image = tf.image.decode_png(image_str, channels=3)\n",
    "    image = tf.image.resize_images(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image_str = tf.read_file(path)\n",
    "    return preprocess_image(image_str)\n",
    "\n",
    "# The tuples are unpacked into the positional arguments of the mapped function\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "    return load_and_preprocess_image(path), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQsXFwViIuBv"
   },
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNa8YF2DIuBv"
   },
   "outputs": [],
   "source": [
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "num_class = len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErlTp0tzeu2o"
   },
   "source": [
    "### Test with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiduSt_3IuBx"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg16.VGG16(\n",
    "                input_shape=input_shape,\n",
    "                include_top=False,\n",
    "                weights='imagenet')\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(num_class, activation='softmax')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5i_TLSxZexmP"
   },
   "source": [
    "### Test with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ob3YLwz4IuBy"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "6CAgOAEuIuB0",
    "outputId": "f370297b-2773-47a3-d4c4-7fc3b75260c2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGczMMxrIuB2"
   },
   "source": [
    "### Test iterate time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0gbMRCMIuB2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(dataset):\n",
    "    overall_start = time.time()\n",
    "    n_image = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    history = model.fit(dataset,\n",
    "                        epochs=1)\n",
    "    ''' \n",
    "    for n_batch, (images, labels) in enumerate(dataset):\n",
    "        if n_batch%10 == 0:\n",
    "            print(\"\\r{} images: {:.2f} s\".format(n_batch * BATCH_SIZE, time.time()-start), \n",
    "                                                 end='', flush=True)\n",
    "    ''' \n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    \n",
    "    print(\"{} images: {:0.2f} s\".format(image_count, duration))\n",
    "    print(\"{:0.5f} Images/s\".format(image_count/float(duration)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwtSHI9CIuB4"
   },
   "source": [
    "# Input pipeline experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssMZiuC1IuB4"
   },
   "source": [
    "## 1. Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "Y1VgmFfUIuB5",
    "outputId": "6c0d521f-454c-4f0e-a0fd-6c6ed2b47db8"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "il7Owmpabr7i"
   },
   "source": [
    "## 2. Prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4bnzUhqIuCI"
   },
   "source": [
    "See more about [tf.data.experimental.prefetch_to_device](https://www.tensorflow.org/api_docs/python/tf/data/experimental/prefetch_to_device)\n",
    "```\n",
    "tf.data.experimental.prefetch_to_device(\n",
    "    device,\n",
    "    buffer_size=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GN0tc3eGIuB6",
    "outputId": "9eae3402-db3a-426a-c81a-3ee6390f6140"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.prefetch(buffer_size=AUTOTUNE) # Only on CPU\n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ldbthqD2IuB8",
    "outputId": "363f798d-5944-4ffc-9d4e-c8175e1a4fa7"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "#image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YiXknLrIuB-"
   },
   "source": [
    "## 3. Map with num_parallel_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cueqngLeIuB-",
    "outputId": "c1e88933-5f01-4a41-8b70-b0480b670026"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZXRGYCbIuCA"
   },
   "source": [
    "## 4. tf.data.experimental.shuffle_and_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhMrJARMIuCA"
   },
   "source": [
    "See more about [tf.data.experimental.shuffle_and_repeat](https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat)\n",
    "```\n",
    "tf.data.experimental.shuffle_and_repeat(\n",
    "    buffer_size,\n",
    "    count=None,\n",
    "    seed=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_a_yfF-_IuCB",
    "outputId": "ba85ecfa-39ea-41fd-a89c-4201f7e4d7e2"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "#path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "#path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "#path_label_ds = path_label_ds.shuffle(buffer_size=image_count).repeat(NUM_EPOCHS)\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55AfEg9LIuCC"
   },
   "source": [
    "## 5. tf.data.experimental.map_and_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VcrjzoCIuCD"
   },
   "source": [
    "See more about [tf.data.experimental.map_and_batch](https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch)\n",
    "```\n",
    "tf.data.experimental.map_and_batch(\n",
    "    map_func,\n",
    "    batch_size,\n",
    "    num_parallel_batches=None,\n",
    "    drop_remainder=False,\n",
    "    num_parallel_calls=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "-gFkJcLxIuCD",
    "outputId": "9b719cd5-0b41-4bba-d3a7-80234cb893b0"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "#image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n",
    "#image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euH0MpThIuCF"
   },
   "source": [
    "## 6. shuffle_and_repeat + map_and_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "w49Cg6uAIuCF",
    "outputId": "281ba9a4-0ef1-4d27-c012-f91a6903b9e4"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yYCBdCmIuCL"
   },
   "source": [
    "## 7. Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQDXKWq8IuCL"
   },
   "source": [
    "See more about [tf.data.Dataset.cache](https://www.tensorflow.org/tutorials/load_data/images#cache)\n",
    "                                       \n",
    "Use tf.data.Dataset.cache to easily cache calculations across epochs. This is especially performant if the dataq fits in memory\n",
    "```\n",
    "ds = image_label_ds.cache()\n",
    "```\n",
    "\n",
    "One disadvantage to using an in memory cache is that the cache must be rebuilt on each run, giving the same startup delay each time the dataset is started:\n",
    "If the data doesn't fit in memory, use a cache file. \n",
    "The cache file also has the advantage that it can be used to quickly restart the dataset without rebuilding the cache. Note how much faster it is the second time:\n",
    "\n",
    "\n",
    "```\n",
    "ds = image_label_ds.cache(filename='./cache.tf-data')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PX3OtYcbIuCM",
    "outputId": "2a1fc11f-fc02-4f9c-8a64-a29b8d4d6100"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "ds = ds.cache(filename='./cache.tf-ds')\n",
    "\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "_-_mZzZnIuCP",
    "outputId": "921b9383-9ac5-428f-9d66-938ec90570f7"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "path_label_ds = path_label_ds.cache(filename='./cache.tf-path')\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "image_label_ds = image_label_ds.cache(filename='./cache.tf-image')\n",
    "\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "“input_pipeline.ipynb”",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
