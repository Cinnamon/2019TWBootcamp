{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yUUhv9uuIuBk",
    "outputId": "4c2c94cd-0aba-45c4-aa75-f4d94f1d3247"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vXeY_Y_eati"
   },
   "source": [
    "If you are using Google Colab, go to data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "g5OxH2yrLa-2",
    "outputId": "ec56071e-8062-40ed-ddf1-a4411b8cbdbc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/2019TWBootcamp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YupbhCORIuBo"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AY7_h0OmIuBo",
    "outputId": "b8038ac7-b0fa-4ee3-8e9c-a4067da9527f"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "#img_path = \"/home/Data/CharactersTrimPad28/\"\n",
    "#img_path = \"./s3mnt/ChineseNumbers/\"\n",
    "img_path = \"/home/Data/ChineseNumbers/\"\n",
    "data_root = pathlib.Path(img_path)\n",
    "if not data_root.exists():\n",
    "    print(\"{} not exist!\".format(data_root))\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 256 \n",
    "IMG_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k95tBs9gIuBq"
   },
   "source": [
    "## Generate filename and label list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ISBR3XBpIuBr",
    "outputId": "08b9115f-dcfc-4c3e-cf8f-b55a85505fcc"
   },
   "outputs": [],
   "source": [
    "# tf.data.Dataset.from_tensor_slices\n",
    "all_image_paths = [str(path) for path in list(data_root.glob('*/*'))]\n",
    "\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "image_count = len(all_image_paths) * NUM_EPOCHS\n",
    "\n",
    "for i in range(3):\n",
    "    print(all_image_paths[i])\n",
    "image_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgRBtZvWIuBs"
   },
   "source": [
    "## Mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVepb_l3IuBt"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_str):\n",
    "    image = tf.image.decode_png(image_str, channels=3)\n",
    "    image = tf.image.resize_images(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image_str = tf.read_file(path)\n",
    "    return preprocess_image(image_str)\n",
    "\n",
    "# The tuples are unpacked into the positional arguments of the mapped function\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "    return load_and_preprocess_image(path), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQsXFwViIuBv"
   },
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNa8YF2DIuBv"
   },
   "outputs": [],
   "source": [
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "num_class = len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErlTp0tzeu2o"
   },
   "source": [
    "### Test with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiduSt_3IuBx"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg16.VGG16(\n",
    "                input_shape=input_shape,\n",
    "                include_top=False,\n",
    "                weights='imagenet')\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(num_class, activation='softmax')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5i_TLSxZexmP"
   },
   "source": [
    "### Test with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ob3YLwz4IuBy"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "6CAgOAEuIuB0",
    "outputId": "f370297b-2773-47a3-d4c4-7fc3b75260c2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGczMMxrIuB2"
   },
   "source": [
    "### Test iterate time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0gbMRCMIuB2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(dataset):\n",
    "    overall_start = time.time()\n",
    "    n_image = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    history = model.fit(dataset,\n",
    "                        epochs=1)\n",
    "    ''' \n",
    "    for n_batch, (images, labels) in enumerate(dataset):\n",
    "        if n_batch%10 == 0:\n",
    "            print(\"\\r{} images: {:.2f} s\".format(n_batch * BATCH_SIZE, time.time()-start), \n",
    "                                                 end='', flush=True)\n",
    "    ''' \n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    \n",
    "    print(\"{} images: {:0.2f} s\".format(image_count, duration))\n",
    "    print(\"{:0.5f} Images/s\".format(image_count/float(duration)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwtSHI9CIuB4"
   },
   "source": [
    "# Input pipeline experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssMZiuC1IuB4"
   },
   "source": [
    "## 1. Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "Y1VgmFfUIuB5",
    "outputId": "6c0d521f-454c-4f0e-a0fd-6c6ed2b47db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74/148 [==============>...............] - ETA: 5:51 - loss: 107.1452 - acc: 0.0476"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a7369fac2328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_label_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-061942fd662c>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     history = model.fit(dataset,\n\u001b[0;32m----> 9\u001b[0;31m                         epochs=1)\n\u001b[0m\u001b[1;32m     10\u001b[0m     ''' \n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3442\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3443\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3444\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    560\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cinnamon/Workspace/Python3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "il7Owmpabr7i"
   },
   "source": [
    "## 2. Prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4bnzUhqIuCI"
   },
   "source": [
    "See more about [tf.data.experimental.prefetch_to_device](https://www.tensorflow.org/api_docs/python/tf/data/experimental/prefetch_to_device)\n",
    "```\n",
    "tf.data.experimental.prefetch_to_device(\n",
    "    device,\n",
    "    buffer_size=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GN0tc3eGIuB6",
    "outputId": "9eae3402-db3a-426a-c81a-3ee6390f6140"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.prefetch(buffer_size=AUTOTUNE) # Only on CPU\n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ldbthqD2IuB8",
    "outputId": "363f798d-5944-4ffc-9d4e-c8175e1a4fa7"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "#image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YiXknLrIuB-"
   },
   "source": [
    "## 3. Map with num_parallel_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cueqngLeIuB-",
    "outputId": "c1e88933-5f01-4a41-8b70-b0480b670026"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZXRGYCbIuCA"
   },
   "source": [
    "## 4. tf.data.experimental.shuffle_and_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhMrJARMIuCA"
   },
   "source": [
    "See more about [tf.data.experimental.shuffle_and_repeat](https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat)\n",
    "```\n",
    "tf.data.experimental.shuffle_and_repeat(\n",
    "    buffer_size,\n",
    "    count=None,\n",
    "    seed=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_a_yfF-_IuCB",
    "outputId": "ba85ecfa-39ea-41fd-a89c-4201f7e4d7e2"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "#path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "#path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "#path_label_ds = path_label_ds.shuffle(buffer_size=image_count).repeat(NUM_EPOCHS)\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55AfEg9LIuCC"
   },
   "source": [
    "## 5. tf.data.experimental.map_and_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VcrjzoCIuCD"
   },
   "source": [
    "See more about [tf.data.experimental.map_and_batch](https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch)\n",
    "```\n",
    "tf.data.experimental.map_and_batch(\n",
    "    map_func,\n",
    "    batch_size,\n",
    "    num_parallel_batches=None,\n",
    "    drop_remainder=False,\n",
    "    num_parallel_calls=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "-gFkJcLxIuCD",
    "outputId": "9b719cd5-0b41-4bba-d3a7-80234cb893b0"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "#image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n",
    "#image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euH0MpThIuCF"
   },
   "source": [
    "## 6. shuffle_and_repeat + map_and_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "w49Cg6uAIuCF",
    "outputId": "281ba9a4-0ef1-4d27-c012-f91a6903b9e4"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yYCBdCmIuCL"
   },
   "source": [
    "## 7. Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQDXKWq8IuCL"
   },
   "source": [
    "See more about [tf.data.Dataset.cache](https://www.tensorflow.org/tutorials/load_data/images#cache)\n",
    "                                       \n",
    "Use tf.data.Dataset.cache to easily cache calculations across epochs. This is especially performant if the dataq fits in memory\n",
    "```\n",
    "ds = image_label_ds.cache()\n",
    "```\n",
    "\n",
    "One disadvantage to using an in memory cache is that the cache must be rebuilt on each run, giving the same startup delay each time the dataset is started:\n",
    "If the data doesn't fit in memory, use a cache file. \n",
    "The cache file also has the advantage that it can be used to quickly restart the dataset without rebuilding the cache. Note how much faster it is the second time:\n",
    "\n",
    "\n",
    "```\n",
    "ds = image_label_ds.cache(filename='./cache.tf-data')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PX3OtYcbIuCM",
    "outputId": "2a1fc11f-fc02-4f9c-8a64-a29b8d4d6100"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "ds = ds.cache(filename='./cache.tf-ds')\n",
    "\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "_-_mZzZnIuCP",
    "outputId": "921b9383-9ac5-428f-9d66-938ec90570f7"
   },
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "path_label_ds = path_label_ds.cache(filename='./cache.tf-path')\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "image_label_ds = image_label_ds.cache(filename='./cache.tf-image')\n",
    "\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=AUTOTUNE)) \n",
    "\n",
    "timeit(image_label_ds)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "“input_pipeline.ipynb”",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
