{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation:\n",
    "\n",
    "Before getting start, we need to install all dependencies first. In order not to mess up the system, we'll use virtual environment here. Open your terminal and let's roll ! ! !\n",
    "```shell\n",
    "\n",
    "#Terminal\n",
    "### create a virtual environment called serve_env ###\n",
    "virtualenv -p python3 serve_env\n",
    "\n",
    "### activate the virtual environment and install all dependencies ###\n",
    "source serve_env/bin/activate\n",
    "\n",
    "### we will use the serving_requirement.txt file here ###\n",
    "pip install -r serving_requirement.txt\n",
    "\n",
    "```\n",
    "\n",
    "The above commands would install all the dependencies in serving_requirement.txt to the environment ‚Äúserve_env‚Äù. \n",
    "\n",
    "![create_env](./pics/create_env.png)\n",
    "![source_env](./pics/source_env.png)\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Of course, you can also use pip to install them one by one.\n",
    "\n",
    "```shell\n",
    "\n",
    "#Terminal\n",
    "### Here only listed some must have dependencies for this hands-on ###\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install tensorflow\n",
    "pip install tensorflow-serving-api\n",
    "pip install grpcio\n",
    "pip install scikit-learn\n",
    "pip install tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "Here, we will use jupyter notebook for this and the following couple of sections. As a result, we need to let our jupyter notebook to be able to use the virtual environment that we just create. (You can also refer to this [link](https://anbasile.github.io/programming/2017/06/25/jupyter-venv/))\n",
    "\n",
    "```shell\n",
    "\n",
    "#Terminal\n",
    "### install ipykernel ###\n",
    "pip install ipykernel\n",
    "\n",
    "### adding serve_env to jupyter kernel ###\n",
    "ipython kernel install --user --name=serve_env\n",
    "\n",
    "### check available kernel of jupyter ###\n",
    "ipython kernelspec list\n",
    "```\n",
    "\n",
    "![env_setup](./pics/env_setup.png)\n",
    "\n",
    "\n",
    "Now our jupyter notebook can use the virtual environment that we just create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data,  Utils Define, and Training\n",
    "\n",
    "In this example, we‚Äôll use the super simple MNIST dataset and naive model structure because dataset and model are not the main point in this hands-on. üòò \n",
    "\n",
    "We won‚Äôt cover this part in detail. So, let's quickly go through this section. üòâ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import necessary dependencies\n",
    "#\n",
    "\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "We'll use the super easy MNIST dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load dataset\n",
    "#\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "y_train = np.eye(10)[y_train]\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utils\n",
    "\n",
    "Define some useful fuctions here. (Don't spend too much time here. It's not the main point of this tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ReduceLROnPlateau\n",
    "#\n",
    "\n",
    "class ReduceLROnPlateau():\n",
    "    def __init__(self, lr, factor, patience, min_lr=1e-10):\n",
    "        self.lr = lr\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.min_loss = None\n",
    "        self.epoch_count = 0\n",
    "\n",
    "    def on_epoch_end(self, val_loss, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss:\n",
    "            self.epoch_count = 0\n",
    "            self.min_loss = val_loss\n",
    "        else:\n",
    "            self.epoch_count += 1\n",
    "\n",
    "        if self.epoch_count == self.patience:\n",
    "            self.lr *= self.factor\n",
    "            self.epoch_count = 0\n",
    "\n",
    "            if self.lr <= self.min_lr:\n",
    "                self.lr = self.min_lr\n",
    "                \n",
    "#\n",
    "# Define Generator\n",
    "#\n",
    "\n",
    "def trainData_generator(x, y, total_batch):\n",
    "    '''\n",
    "    Generator that generate training data in batch\n",
    "    Args:\n",
    "        x: input data\n",
    "        h: input label\n",
    "        total_batch: number of batches\n",
    "    '''\n",
    "    assert len(x)/total_batch == int(len(x)/total_batch)\n",
    "    new_ind = shuffle(range(len(x)))\n",
    "    x = x[new_ind]\n",
    "    x = x/255.\n",
    "    y = y[new_ind]\n",
    "    x_batches = np.split(x, total_batch)\n",
    "    y_batches = np.split(y, total_batch)\n",
    "    for batch in range(len(x_batches)):\n",
    "        yield x_batches[batch], y_batches[batch]\n",
    "\n",
    "\n",
    "def validData_generator(x, y):\n",
    "    '''\n",
    "    Function to generate validation data pairs (data, label)\n",
    "    Args:\n",
    "        x: input data\n",
    "        y: input label\n",
    "    '''\n",
    "    new_ind = shuffle(range(len(x)))\n",
    "    x = x[new_ind]\n",
    "    x = x/255.\n",
    "    y = y[new_ind]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Graph\n",
    "\n",
    "The model structure is also really naive. It just contains one convolution-polling and on dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0627 23:47:03.716828 4515165632 deprecation.py:323] From <ipython-input-4-3a5df2585d13>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0627 23:47:03.724770 4515165632 deprecation.py:506] From /Users/admin/Documents/Cinnamon/Bootcamp/Serving_Example/serve_env/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0627 23:47:03.981215 4515165632 deprecation.py:323] From <ipython-input-4-3a5df2585d13>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0627 23:47:04.411651 4515165632 deprecation.py:323] From <ipython-input-4-3a5df2585d13>:32: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0627 23:47:04.875514 4515165632 deprecation.py:323] From <ipython-input-4-3a5df2585d13>:46: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Define model Graph\n",
    "#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "main_graph = tf.Graph()\n",
    "\n",
    "with main_graph.as_default():\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        inputs = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                                None, 28, 28, 1], name='x_input')\n",
    "        y_true = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                                None, 10], name='y_true')\n",
    "        lr = tf.placeholder(dtype=tf.float32, shape=None, name='learning_rate')\n",
    "\n",
    "    with tf.variable_scope('hidden_layers'):\n",
    "        model = tf.layers.conv2d(inputs=inputs,\n",
    "                                 filters=16,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 strides=(1, 1),\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 )\n",
    "        model = tf.layers.max_pooling2d(inputs=model,\n",
    "                                        pool_size=(2, 2),\n",
    "                                        strides=2,\n",
    "                                        )\n",
    "\n",
    "    with tf.variable_scope('output_layer'):\n",
    "        model = tf.layers.Flatten()(model)\n",
    "        output = tf.layers.dense(inputs=model, units=10)\n",
    "        prediction = tf.nn.softmax(output)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output,\n",
    "                                                                         labels=y_true,\n",
    "                                                                         name='cross_entropy_loss'))\n",
    "\n",
    "        optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        gra_and_var = optim.compute_gradients(loss)\n",
    "        update = optim.apply_gradients(gra_and_var)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_predict = tf.equal(tf.arg_max(\n",
    "            prediction, dimension=1), tf.arg_max(y_true, 1))\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predict, dtype=tf.float32), name='accuracy')\n",
    "\n",
    "    init_main_graph = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Hyperparameters\n",
    "\n",
    "We only train one epoch here with batch size equals to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyperparameters\n",
    "#\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 10000\n",
    "total_batch = int(len(x_train) / batch_size)\n",
    "reduceLR = ReduceLROnPlateau(lr=0.001, factor=0.5, patience=10)\n",
    "train_loss, train_acc = [], []\n",
    "valid_loss, valid_acc = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1: tr_loss 2.181, tr_acc 0.375; val_loss 1.998, val_acc 0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "\n",
    "\n",
    "sess = tf.Session(graph=main_graph)\n",
    "sess.run(init_main_graph)\n",
    "\n",
    "for epc in tqdm.tqdm(range(epochs)):\n",
    "    tr_loss_tmp, tr_acc_tmp = 0, 0\n",
    "    val_loss_tmp, val_acc_tmp = 0, 0\n",
    "    train_gen = trainData_generator(x_train, y_train, total_batch=total_batch)\n",
    "\n",
    "    ### training ###\n",
    "    for x_batch, y_batch in train_gen:\n",
    "\n",
    "        tr_acc_batch, tr_loss_batch, _ = sess.run([accuracy, loss, update], feed_dict={\n",
    "            inputs: x_batch,\n",
    "            y_true: y_batch,\n",
    "            lr: reduceLR.lr\n",
    "        })\n",
    "\n",
    "        tr_loss_tmp += tr_loss_batch\n",
    "        tr_acc_tmp += tr_acc_batch\n",
    "\n",
    "    train_loss.append(tr_loss_tmp / total_batch)\n",
    "    train_acc.append(tr_acc_tmp / total_batch)\n",
    "\n",
    "    ### validation ###\n",
    "    x_batch, y_batch = validData_generator(x_test, y_test)\n",
    "\n",
    "    val_acc_batch, val_loss_batch = sess.run([accuracy, loss], feed_dict={\n",
    "        inputs: x_batch,\n",
    "        y_true: y_batch\n",
    "    })\n",
    "\n",
    "    val_loss_tmp += val_loss_batch\n",
    "    val_acc_tmp += val_acc_batch\n",
    "\n",
    "    valid_loss.append(val_loss_tmp)\n",
    "    valid_acc.append(val_acc_tmp)\n",
    "    reduceLR.on_epoch_end(valid_loss[-1])\n",
    "\n",
    "    print(' Epoch {}: tr_loss {:.3f}, tr_acc {:.3f}; val_loss {:.3f}, val_acc {:.3f}'.format(epc+1,\n",
    "                                                                                             train_loss[-1],\n",
    "                                                                                             train_acc[-1],\n",
    "                                                                                             valid_loss[-1],\n",
    "                                                                                             valid_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.9976853132247925], [0.5863000154495239])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Checking our validation loss and accuracy\n",
    "#\n",
    "\n",
    "valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Trained Model:\n",
    "\n",
    "In this section, we will see the most common two ways to save and restore the trained model in TensorFlow. (You can refer to Deployment Introduction‚Äôs [Exporting part](https://paper.dropbox.com/doc/Deployment-Introduction--AgMfk42UNCSVDYgtk2pHmqKdAg-TrYTDU739OGQAP6iyidcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Saver: Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save Model (.ckpt)\n",
    "#\n",
    "\n",
    "with main_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, './excellentModel/myExcellentModel.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving, we should be able to see the excellentModel folder containing our model‚Äôs checkpoint file.\n",
    "\n",
    "![saver_ckpt](./pics/saver_ckpt.png)\n",
    "\n",
    "\n",
    "Let‚Äôs see what each file means.\n",
    "- **meta file**: describes the saved graph structure, including GraphDef, SaveDef, and so on\n",
    "\n",
    "\n",
    "- **index file**: a string-string immutable table (tensor name: metadata of tensor)\n",
    "\n",
    "\n",
    "- **data file**: saves the values of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0627 23:48:38.199552 4515165632 deprecation.py:323] From /Users/admin/Documents/Cinnamon/Bootcamp/Serving_Example/serve_env/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/x_input\n",
      "input/y_true\n",
      "input/learning_rate\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform/shape\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform/min\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform/max\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform/sub\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform/mul\n",
      "hidden_layers/conv2d/kernel/Initializer/random_uniform\n",
      "hidden_layers/conv2d/kernel\n",
      "hidden_layers/conv2d/kernel/Assign\n",
      "hidden_layers/conv2d/kernel/read\n",
      "hidden_layers/conv2d/bias/Initializer/zeros\n",
      "hidden_layers/conv2d/bias\n",
      "hidden_layers/conv2d/bias/Assign\n",
      "hidden_layers/conv2d/bias/read\n",
      "hidden_layers/conv2d/dilation_rate\n",
      "hidden_layers/conv2d/Conv2D\n",
      "hidden_layers/conv2d/BiasAdd\n",
      "hidden_layers/conv2d/Relu\n",
      "hidden_layers/max_pooling2d/MaxPool\n",
      "output_layer/flatten/Shape\n",
      "output_layer/flatten/strided_slice/stack\n",
      "output_layer/flatten/strided_slice/stack_1\n",
      "output_layer/flatten/strided_slice/stack_2\n",
      "output_layer/flatten/strided_slice\n",
      "output_layer/flatten/Reshape/shape/1\n",
      "output_layer/flatten/Reshape/shape\n",
      "output_layer/flatten/Reshape\n",
      "output_layer/dense/kernel/Initializer/random_uniform/shape\n",
      "output_layer/dense/kernel/Initializer/random_uniform/min\n",
      "output_layer/dense/kernel/Initializer/random_uniform/max\n",
      "output_layer/dense/kernel/Initializer/random_uniform/RandomUniform\n",
      "output_layer/dense/kernel/Initializer/random_uniform/sub\n",
      "output_layer/dense/kernel/Initializer/random_uniform/mul\n",
      "output_layer/dense/kernel/Initializer/random_uniform\n",
      "output_layer/dense/kernel\n",
      "output_layer/dense/kernel/Assign\n",
      "output_layer/dense/kernel/read\n",
      "output_layer/dense/bias/Initializer/zeros\n",
      "output_layer/dense/bias\n",
      "output_layer/dense/bias/Assign\n",
      "output_layer/dense/bias/read\n",
      "output_layer/dense/MatMul\n",
      "output_layer/dense/BiasAdd\n",
      "output_layer/Softmax\n",
      "loss/cross_entropy_loss/Rank\n",
      "loss/cross_entropy_loss/Shape\n",
      "loss/cross_entropy_loss/Rank_1\n",
      "loss/cross_entropy_loss/Shape_1\n",
      "loss/cross_entropy_loss/Sub/y\n",
      "loss/cross_entropy_loss/Sub\n",
      "loss/cross_entropy_loss/Slice/begin\n",
      "loss/cross_entropy_loss/Slice/size\n",
      "loss/cross_entropy_loss/Slice\n",
      "loss/cross_entropy_loss/concat/values_0\n",
      "loss/cross_entropy_loss/concat/axis\n",
      "loss/cross_entropy_loss/concat\n",
      "loss/cross_entropy_loss/Reshape\n",
      "loss/cross_entropy_loss/Rank_2\n",
      "loss/cross_entropy_loss/Shape_2\n",
      "loss/cross_entropy_loss/Sub_1/y\n",
      "loss/cross_entropy_loss/Sub_1\n",
      "loss/cross_entropy_loss/Slice_1/begin\n",
      "loss/cross_entropy_loss/Slice_1/size\n",
      "loss/cross_entropy_loss/Slice_1\n",
      "loss/cross_entropy_loss/concat_1/values_0\n",
      "loss/cross_entropy_loss/concat_1/axis\n",
      "loss/cross_entropy_loss/concat_1\n",
      "loss/cross_entropy_loss/Reshape_1\n",
      "loss/cross_entropy_loss\n",
      "loss/cross_entropy_loss/Sub_2/y\n",
      "loss/cross_entropy_loss/Sub_2\n",
      "loss/cross_entropy_loss/Slice_2/begin\n",
      "loss/cross_entropy_loss/Slice_2/size\n",
      "loss/cross_entropy_loss/Slice_2\n",
      "loss/cross_entropy_loss/Reshape_2\n",
      "loss/Const\n",
      "loss/Mean\n",
      "loss/gradients/Shape\n",
      "loss/gradients/grad_ys_0\n",
      "loss/gradients/Fill\n",
      "loss/gradients/loss/Mean_grad/Reshape/shape\n",
      "loss/gradients/loss/Mean_grad/Reshape\n",
      "loss/gradients/loss/Mean_grad/Shape\n",
      "loss/gradients/loss/Mean_grad/Tile\n",
      "loss/gradients/loss/Mean_grad/Shape_1\n",
      "loss/gradients/loss/Mean_grad/Shape_2\n",
      "loss/gradients/loss/Mean_grad/Const\n",
      "loss/gradients/loss/Mean_grad/Prod\n",
      "loss/gradients/loss/Mean_grad/Const_1\n",
      "loss/gradients/loss/Mean_grad/Prod_1\n",
      "loss/gradients/loss/Mean_grad/Maximum/y\n",
      "loss/gradients/loss/Mean_grad/Maximum\n",
      "loss/gradients/loss/Mean_grad/floordiv\n",
      "loss/gradients/loss/Mean_grad/Cast\n",
      "loss/gradients/loss/Mean_grad/truediv\n",
      "loss/gradients/loss/cross_entropy_loss/Reshape_2_grad/Shape\n",
      "loss/gradients/loss/cross_entropy_loss/Reshape_2_grad/Reshape\n",
      "loss/gradients/zeros_like\n",
      "loss/gradients/loss/cross_entropy_loss_grad/ExpandDims/dim\n",
      "loss/gradients/loss/cross_entropy_loss_grad/ExpandDims\n",
      "loss/gradients/loss/cross_entropy_loss_grad/mul\n",
      "loss/gradients/loss/cross_entropy_loss_grad/LogSoftmax\n",
      "loss/gradients/loss/cross_entropy_loss_grad/Neg\n",
      "loss/gradients/loss/cross_entropy_loss_grad/ExpandDims_1/dim\n",
      "loss/gradients/loss/cross_entropy_loss_grad/ExpandDims_1\n",
      "loss/gradients/loss/cross_entropy_loss_grad/mul_1\n",
      "loss/gradients/loss/cross_entropy_loss_grad/tuple/group_deps\n",
      "loss/gradients/loss/cross_entropy_loss_grad/tuple/control_dependency\n",
      "loss/gradients/loss/cross_entropy_loss_grad/tuple/control_dependency_1\n",
      "loss/gradients/loss/cross_entropy_loss/Reshape_grad/Shape\n",
      "loss/gradients/loss/cross_entropy_loss/Reshape_grad/Reshape\n",
      "loss/gradients/output_layer/dense/BiasAdd_grad/BiasAddGrad\n",
      "loss/gradients/output_layer/dense/BiasAdd_grad/tuple/group_deps\n",
      "loss/gradients/output_layer/dense/BiasAdd_grad/tuple/control_dependency\n",
      "loss/gradients/output_layer/dense/BiasAdd_grad/tuple/control_dependency_1\n",
      "loss/gradients/output_layer/dense/MatMul_grad/MatMul\n",
      "loss/gradients/output_layer/dense/MatMul_grad/MatMul_1\n",
      "loss/gradients/output_layer/dense/MatMul_grad/tuple/group_deps\n",
      "loss/gradients/output_layer/dense/MatMul_grad/tuple/control_dependency\n",
      "loss/gradients/output_layer/dense/MatMul_grad/tuple/control_dependency_1\n",
      "loss/gradients/output_layer/flatten/Reshape_grad/Shape\n",
      "loss/gradients/output_layer/flatten/Reshape_grad/Reshape\n",
      "loss/gradients/hidden_layers/max_pooling2d/MaxPool_grad/MaxPoolGrad\n",
      "loss/gradients/hidden_layers/conv2d/Relu_grad/ReluGrad\n",
      "loss/gradients/hidden_layers/conv2d/BiasAdd_grad/BiasAddGrad\n",
      "loss/gradients/hidden_layers/conv2d/BiasAdd_grad/tuple/group_deps\n",
      "loss/gradients/hidden_layers/conv2d/BiasAdd_grad/tuple/control_dependency\n",
      "loss/gradients/hidden_layers/conv2d/BiasAdd_grad/tuple/control_dependency_1\n",
      "loss/gradients/hidden_layers/conv2d/Conv2D_grad/ShapeN\n",
      "loss/gradients/hidden_layers/conv2d/Conv2D_grad/Conv2DBackpropInput\n",
      "loss/gradients/hidden_layers/conv2d/Conv2D_grad/Conv2DBackpropFilter\n",
      "loss/gradients/hidden_layers/conv2d/Conv2D_grad/tuple/group_deps\n",
      "loss/gradients/hidden_layers/conv2d/Conv2D_grad/tuple/control_dependency\n",
      "loss/gradients/hidden_layers/conv2d/Conv2D_grad/tuple/control_dependency_1\n",
      "loss/beta1_power/initial_value\n",
      "loss/beta1_power\n",
      "loss/beta1_power/Assign\n",
      "loss/beta1_power/read\n",
      "loss/beta2_power/initial_value\n",
      "loss/beta2_power\n",
      "loss/beta2_power/Assign\n",
      "loss/beta2_power/read\n",
      "hidden_layers/conv2d/kernel/Adam/Initializer/zeros\n",
      "hidden_layers/conv2d/kernel/Adam\n",
      "hidden_layers/conv2d/kernel/Adam/Assign\n",
      "hidden_layers/conv2d/kernel/Adam/read\n",
      "hidden_layers/conv2d/kernel/Adam_1/Initializer/zeros\n",
      "hidden_layers/conv2d/kernel/Adam_1\n",
      "hidden_layers/conv2d/kernel/Adam_1/Assign\n",
      "hidden_layers/conv2d/kernel/Adam_1/read\n",
      "hidden_layers/conv2d/bias/Adam/Initializer/zeros\n",
      "hidden_layers/conv2d/bias/Adam\n",
      "hidden_layers/conv2d/bias/Adam/Assign\n",
      "hidden_layers/conv2d/bias/Adam/read\n",
      "hidden_layers/conv2d/bias/Adam_1/Initializer/zeros\n",
      "hidden_layers/conv2d/bias/Adam_1\n",
      "hidden_layers/conv2d/bias/Adam_1/Assign\n",
      "hidden_layers/conv2d/bias/Adam_1/read\n",
      "output_layer/dense/kernel/Adam/Initializer/zeros/shape_as_tensor\n",
      "output_layer/dense/kernel/Adam/Initializer/zeros/Const\n",
      "output_layer/dense/kernel/Adam/Initializer/zeros\n",
      "output_layer/dense/kernel/Adam\n",
      "output_layer/dense/kernel/Adam/Assign\n",
      "output_layer/dense/kernel/Adam/read\n",
      "output_layer/dense/kernel/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "output_layer/dense/kernel/Adam_1/Initializer/zeros/Const\n",
      "output_layer/dense/kernel/Adam_1/Initializer/zeros\n",
      "output_layer/dense/kernel/Adam_1\n",
      "output_layer/dense/kernel/Adam_1/Assign\n",
      "output_layer/dense/kernel/Adam_1/read\n",
      "output_layer/dense/bias/Adam/Initializer/zeros\n",
      "output_layer/dense/bias/Adam\n",
      "output_layer/dense/bias/Adam/Assign\n",
      "output_layer/dense/bias/Adam/read\n",
      "output_layer/dense/bias/Adam_1/Initializer/zeros\n",
      "output_layer/dense/bias/Adam_1\n",
      "output_layer/dense/bias/Adam_1/Assign\n",
      "output_layer/dense/bias/Adam_1/read\n",
      "loss/Adam/beta1\n",
      "loss/Adam/beta2\n",
      "loss/Adam/epsilon\n",
      "loss/Adam/update_hidden_layers/conv2d/kernel/ApplyAdam\n",
      "loss/Adam/update_hidden_layers/conv2d/bias/ApplyAdam\n",
      "loss/Adam/update_output_layer/dense/kernel/ApplyAdam\n",
      "loss/Adam/update_output_layer/dense/bias/ApplyAdam\n",
      "loss/Adam/mul\n",
      "loss/Adam/Assign\n",
      "loss/Adam/mul_1\n",
      "loss/Adam/Assign_1\n",
      "loss/Adam\n",
      "accuracy/ArgMax/dimension\n",
      "accuracy/ArgMax\n",
      "accuracy/ArgMax_1/dimension\n",
      "accuracy/ArgMax_1\n",
      "accuracy/Equal\n",
      "accuracy/Cast\n",
      "accuracy/Const\n",
      "accuracy/accuracy\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/Assign_12\n",
      "save/Assign_13\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load saved Model (.ckpt)\n",
    "#\n",
    "\n",
    "\"\"\" create new graph \"\"\"\n",
    "tf.reset_default_graph()\n",
    "ckpt_graph = tf.Graph()\n",
    "ckpt_sess = tf.Session(graph=ckpt_graph)\n",
    "\n",
    "with ckpt_graph.as_default():\n",
    "    saver = tf.train.import_meta_graph(\n",
    "        './excellentModel/myExcellentModel.ckpt.meta')\n",
    "    saver.restore(ckpt_sess, './excellentModel/myExcellentModel.ckpt')    \n",
    "\n",
    "for node in ckpt_graph.as_graph_def().node:\n",
    "    print(node.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9976853, 0.5863)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Get tensors and check accuracy and loss\n",
    "#\n",
    "\n",
    "# If you want to retrained this model, you need to get the tensors back by their names.\n",
    "\n",
    "ckpt_acc = ckpt_sess.graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "ckpt_loss = ckpt_sess.graph.get_tensor_by_name('loss/Mean:0')\n",
    "ckpt_input = ckpt_sess.graph.get_tensor_by_name('input/x_input:0')\n",
    "ckpt_true = ckpt_sess.graph.get_tensor_by_name('input/y_true:0')\n",
    "ckpt_out = ckpt_sess.graph.get_tensor_by_name('output_layer/Softmax:0')\n",
    "\n",
    "acc_ckpt, loss_ckpt, pred_ckpt = ckpt_sess.run([ckpt_acc, ckpt_loss, ckpt_out], feed_dict={ckpt_input: x_batch, \n",
    "                                                                                           ckpt_true: y_batch})\n",
    "\n",
    "loss_ckpt, acc_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use [SaveModel](https://paper.dropbox.com/doc/Deployment-Introduction--AgMfk42UNCSVDYgtk2pHmqKdAg-TrYTDU739OGQAP6iyidcc): Protobuf\n",
    "\n",
    "Using Saver, you have to know the name of the model‚Äôs output and input tensors. Sometimes it will be super tedious if there is no standard naming policy. As a result, here we will use another way to export the model to Protobuf, and this is also the **standard ways to export TensorFlow models for serving**.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Here, we will go through three different scenarios. One is using **saved model builder without signature**, another one is using **saved model builder with signature**, and the other is using **simple save**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Export Model with Saved Model Builder without Signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exporting model to protobuf without defining SignatureDef\n",
    "#\n",
    "\n",
    "export_path = './Saved_Model_noSig/1'\n",
    "with ckpt_graph.as_default():\n",
    "    builder_noSig = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "    builder_noSig.add_meta_graph_and_variables(\n",
    "        ckpt_sess, ['noSig_tag1'])\n",
    "    builder_noSig.add_meta_graph(['noSig_tag2'])\n",
    "    builder_noSig.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should be able to see the exported Protobuf Model.\n",
    "\n",
    "![saved_model_nosig](./pics/saved_model_nosig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the command **saved_model_cli** to check the Protobuf Model information. You can install it from [source](https://www.tensorflow.org/guide/saved_model#install_the_savedmodel_cli). However, it should already be installed if we use pip install to install TensorFlow.\n",
    "\n",
    "```shell\n",
    "#Terminal\n",
    "saved_model_cli show --dir ./Saved_Model_noSig/1 --all\n",
    "```\n",
    "\n",
    "![saved_model_nosig_cli](./pics/saved_model_nosig_cli.png)\n",
    "\n",
    "There is not much information because we didn‚Äôt set signature here. We can only see tag-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the exported model. \n",
    "Because we save two graphs in the same Protobuf, we can load both of them with respective tag names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0627 23:48:43.701056 4515165632 deprecation.py:323] From <ipython-input-12-7e1cd9eabd79>:15: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.9976853, 0.5863)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Load Model from .pb without Signature\n",
    "#\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "noSig_graph1 = tf.Graph()\n",
    "noSig_graph2 = tf.Graph()\n",
    "noSig_sess1 = tf.Session(graph=noSig_graph1)\n",
    "noSig_sess2 = tf.Session(graph=noSig_graph2)\n",
    "\n",
    "\n",
    "\"\"\" Load noSig_tag1 \"\"\"\n",
    "with noSig_graph1.as_default():\n",
    "    meta_graph_noSignature1 = tf.saved_model.loader.load(\n",
    "        noSig_sess1, ['noSig_tag1'], export_path)\n",
    "\n",
    "\"\"\" Load noSig_tag2 \"\"\"\n",
    "with noSig_graph2.as_default():\n",
    "    meta_graph_noSignature2 = tf.saved_model.loader.load(\n",
    "        noSig_sess2, ['noSig_tag2'], export_path)\n",
    "\n",
    "### Get Accuracy and Loss ###\n",
    "noSig_acc1 = noSig_sess1.graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "noSig_loss1 = noSig_sess1.graph.get_tensor_by_name('loss/Mean:0')\n",
    "noSig_input1 = noSig_sess1.graph.get_tensor_by_name('input/x_input:0')\n",
    "noSig_true1 = noSig_sess1.graph.get_tensor_by_name('input/y_true:0')\n",
    "noSig_out1 = noSig_sess1.graph.get_tensor_by_name('output_layer/Softmax:0')\n",
    "\n",
    "acc_noSig1, loss_noSig1, noSig_pred1 = noSig_sess1.run([noSig_acc1, noSig_loss1, noSig_out1], feed_dict={\n",
    "    noSig_input1: x_batch,\n",
    "    noSig_true1: y_batch\n",
    "})\n",
    "\n",
    "loss_noSig1, acc_noSig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we still have to know the name of the model‚Äôs output and input. That‚Äôs because we didn‚Äôt define signature before exporting the model.\n",
    "\n",
    "Now, let‚Äôs see what signature do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Export Model with Saved Model Builder with Signature:\n",
    "\n",
    "Notice that before exporting, we need to transform the tensors to **TensorInfo protocol buffer** which will be used by signature definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export Model to .pb with defining SignatureDef\n",
    "#\n",
    "\n",
    "export_path = './Saved_Model_withSig/1'\n",
    "builder_withSig = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "\"\"\" Get tensor info from input and output tensors: transform tensor to TensorInfo protocol buff \"\"\"\n",
    "input_withSig = tf.saved_model.utils.build_tensor_info(ckpt_input)\n",
    "label_withSig = tf.saved_model.utils.build_tensor_info(ckpt_true)\n",
    "acc_withSig = tf.saved_model.utils.build_tensor_info(ckpt_acc)\n",
    "loss_withSig = tf.saved_model.utils.build_tensor_info(ckpt_loss)\n",
    "pred_withSig = tf.saved_model.utils.build_tensor_info(ckpt_out)\n",
    "\n",
    "\"\"\" Define signature_definition: define input and output proto \"\"\"\n",
    "# a signature is the set of inputs to and outputs from a graph\n",
    "signature_definition = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={\n",
    "        'input_x': input_withSig,\n",
    "        'input_label': label_withSig\n",
    "    },\n",
    "    outputs={\n",
    "        'accuracy': acc_withSig,\n",
    "        'loss': loss_withSig,\n",
    "        'softmaxOut': pred_withSig\n",
    "    },\n",
    "    method_name='withSig_method_name' #self-defined method_name\n",
    ")\n",
    "\n",
    "\"\"\" export \"\"\"\n",
    "with ckpt_graph.as_default():\n",
    "    builder_withSig.add_meta_graph_and_variables(ckpt_sess,\n",
    "                                                 ['withSig_tag'],\n",
    "                                                 signature_def_map={'withSig_Key': signature_definition}) #self-defined signature key\n",
    "    builder_withSig.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we need to convert input and output tensors to TensorInfo object first. Then, we can define *signature_definition*. \n",
    "\n",
    "**The logical keys of inputs and outputs are self-defined.**\n",
    "\n",
    "\n",
    "Also, we can specify our own **method_name**. (Be aware that when using Predict API, we usually use pre-defined predict method constant)\n",
    "\n",
    "\n",
    "The tag and the key to signature_definition are both self-defined. Like above, we also usually **used pre-defined serving tag-constant and signature-constant for Serving.**\n",
    "\n",
    "\n",
    "Now, we should be able to see the exported model.\n",
    "\n",
    "![saved_model_withsig](./pics/saved_model_withsig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, let's check exported model with **saved_model_cli**\n",
    "\n",
    "```shell\n",
    "#terminal\n",
    "saved_model_cli show --dir Saved_Model_withSig/1 --all\n",
    "```\n",
    "\n",
    "![saved_model_withsig_cli](./pics/saved_model_withsig_cli.png)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Now we can see all the detailed information about the model that we just exported. We can see the **inputs keys**, **outputs keys**, **method name**, **signature key**, etc.\n",
    "\n",
    "#### All these information will be used for client when making request to the server ! ! !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature: \n",
      "\n",
      "{'withSig_Key': inputs {\n",
      "  key: \"input_label\"\n",
      "  value {\n",
      "    name: \"input/y_true:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"input_x\"\n",
      "  value {\n",
      "    name: \"input/x_input:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"accuracy\"\n",
      "  value {\n",
      "    name: \"accuracy/accuracy:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"loss\"\n",
      "  value {\n",
      "    name: \"loss/Mean:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"softmaxOut\"\n",
      "  value {\n",
      "    name: \"output_layer/Softmax:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "method_name: \"withSig_method_name\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load Model from .pb with defining SignatureDef\n",
    "#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "withSig_graph = tf.Graph()\n",
    "withSig_sess = tf.Session(graph=withSig_graph)\n",
    "\n",
    "\n",
    "with withSig_graph.as_default():\n",
    "    meta_graph_withSig = tf.saved_model.loader.load(\n",
    "        withSig_sess, ['withSig_tag'], export_path)\n",
    "    signature = meta_graph_withSig.signature_def\n",
    "    \n",
    "print('Signature: \\n')\n",
    "print(signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With signature, we can easily see the names of inputs, outputs, and their corresponding keys.\n",
    "\n",
    "Having all those information, we can retrieve the input and output tensors‚Äô names by the logical keys stored in the signature.\n",
    "\n",
    "(**No need to search through the whole tensors to look for the right names anymore ! ! ! !**ü§ó )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9976853, 0.5863)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting names for desired tensors ###\n",
    "x_name = signature['withSig_Key'].inputs['input_x'].name\n",
    "label_name = signature['withSig_Key'].inputs['input_label'].name\n",
    "acc_name = signature['withSig_Key'].outputs['accuracy'].name\n",
    "loss_name = signature['withSig_Key'].outputs['loss'].name\n",
    "softmaxOut_name = signature['withSig_Key'].outputs['softmaxOut'].name\n",
    "\n",
    "### Getting tensors by names ###\n",
    "withSig_input = withSig_sess.graph.get_tensor_by_name(x_name)\n",
    "withSig_true = withSig_sess.graph.get_tensor_by_name(label_name)\n",
    "withSig_acc = withSig_sess.graph.get_tensor_by_name(acc_name)\n",
    "withSig_loss = withSig_sess.graph.get_tensor_by_name(loss_name)\n",
    "withSig_pred = withSig_sess.graph.get_tensor_by_name(softmaxOut_name)\n",
    "\n",
    "### Testing ###\n",
    "acc_withSig, loss_withSig, pred_withSig = withSig_sess.run([withSig_acc, withSig_loss, withSig_pred], feed_dict={\n",
    "    withSig_input: x_batch,\n",
    "    withSig_true: y_batch\n",
    "})\n",
    "\n",
    "loss_withSig, acc_withSig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Export Model with Simple Save:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple save method is pretty easy. But, there are a few things we need to be aware of.\n",
    "If we use **simple_save** to export the model:\n",
    "\n",
    "1. The default tag is **\"serve\"** (tf.saved_model.tag_constants.SERVING)\n",
    "\n",
    "2. The signature key is **\"serving_default\"** (tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n",
    "\n",
    "3. The method_name for signature_def is **\"tensorflow/serving/predict\"** (tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export Model to .pb with simple_save\n",
    "#\n",
    "export_path = './Saved_Model_SimpleSave/1'\n",
    "\n",
    "### export ###\n",
    "with ckpt_graph.as_default():\n",
    "    tf.saved_model.simple_save(\n",
    "        ckpt_sess,\n",
    "        export_path,\n",
    "        inputs={'input_x': ckpt_input, 'input_label': ckpt_true},\n",
    "        outputs={'accuracy': ckpt_acc, 'loss': ckpt_loss, 'softmaxOut': ckpt_out}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should be able to see the exported model.\n",
    "\n",
    "![saved_model_simplesave](./pics/saved_model_simplesave.png)\n",
    "\n",
    "\n",
    "Let's check exported model by **saved_model_cli**.\n",
    "```shell\n",
    "#terminal\n",
    "saved_model_cli show --dir Saved_Model_SimpleSave/1 --all\n",
    "```\n",
    "\n",
    "![saved_model_simplesave_cli](./pics/saved_model_simplesave_cli.png)\n",
    "\n",
    "\n",
    "As mentioned above, the tag, signature key, and method_name will all use default serving constants. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can get all the information we need via the meta graph‚Äôs signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature:\n",
      "{'serving_default': inputs {\n",
      "  key: \"input_label\"\n",
      "  value {\n",
      "    name: \"input/y_true:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"input_x\"\n",
      "  value {\n",
      "    name: \"input/x_input:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"accuracy\"\n",
      "  value {\n",
      "    name: \"accuracy/accuracy:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"loss\"\n",
      "  value {\n",
      "    name: \"loss/Mean:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"softmaxOut\"\n",
      "  value {\n",
      "    name: \"output_layer/Softmax:0\"\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "method_name: \"tensorflow/serving/predict\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load Model from .pb exported by simple_save\n",
    "#\n",
    "tf.reset_default_graph()\n",
    "withSig_graph = tf.Graph()\n",
    "withSig_sess = tf.Session(graph=withSig_graph)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "simpleSave_graph = tf.Graph()\n",
    "simpleSavfe_sess = tf.Session(graph=simpleSave_graph)\n",
    "\n",
    "with simpleSave_graph.as_default():\n",
    "    meta_graph_simpleSave = tf.saved_model.loader.load(simpleSavfe_sess,\n",
    "                                                       [tf.saved_model.tag_constants.SERVING], #tag: serve\n",
    "                                                       export_path)\n",
    "    \n",
    "    signature_simpleSave = meta_graph_simpleSave.signature_def\n",
    "    \n",
    "print('Signature:')\n",
    "print(signature_simpleSave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve input and output tensors‚Äô names by the local keys that we assigned in the signature. \n",
    "\n",
    "Just remember the signature key here is **tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9976853, 0.5863)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting names for desired tensors ###\n",
    "x_name = signature_simpleSave['serving_default'].inputs['input_x'].name\n",
    "label_name = signature_simpleSave['serving_default'].inputs['input_label'].name\n",
    "acc_name = signature_simpleSave['serving_default'].outputs['accuracy'].name\n",
    "loss_name = signature_simpleSave['serving_default'].outputs['loss'].name\n",
    "softmaxOut_name = signature_simpleSave['serving_default'].outputs['softmaxOut'].name\n",
    "\n",
    "### Getting tensors by names ###\n",
    "simpleSave_input = simpleSavfe_sess.graph.get_tensor_by_name(x_name)\n",
    "simpleSave_true = simpleSavfe_sess.graph.get_tensor_by_name(label_name)\n",
    "simpleSave_acc = simpleSavfe_sess.graph.get_tensor_by_name(acc_name)\n",
    "simpleSave_loss = simpleSavfe_sess.graph.get_tensor_by_name(loss_name)\n",
    "simpleSave_pred = simpleSavfe_sess.graph.get_tensor_by_name(softmaxOut_name)\n",
    "\n",
    "### Testing ###\n",
    "acc_simpleSave, loss_simpleSave, pred_simpleSave = simpleSavfe_sess.run(\n",
    "    [simpleSave_acc, simpleSave_loss, simpleSave_pred],\n",
    "    feed_dict={\n",
    "                simpleSave_input: x_batch,\n",
    "                simpleSave_true: y_batch\n",
    "    })\n",
    "\n",
    "loss_simpleSave, acc_simpleSave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model for Serving\n",
    "\n",
    "So far, we know many ways to export or save our trained model. Now, let's really export our model for serving.\n",
    "\n",
    "There are a few things that need to be aware of.\n",
    "\n",
    "1. We used **tf.saved_model.signature_constants.PREDICT_METHOD_NAME ** method name here because we will use **predict API** to make request in client section.\n",
    "\n",
    "\n",
    "\n",
    "2. The exported model will be used for serving, so we assign the tag to be **tf.saved_model.tag_constants.SERVING**\n",
    "\n",
    "\n",
    "\n",
    "3. Like the above, we assign the key of signature definition to be **tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export Model for serving\n",
    "#\n",
    "export_path = './Saved_Model/1'\n",
    "serving_builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "\"\"\" Get tensor info from input and output tensors: transform tensor to TensorInfo protocol buff \"\"\"\n",
    "serving_input = tf.saved_model.utils.build_tensor_info(ckpt_input)\n",
    "serving_label = tf.saved_model.utils.build_tensor_info(ckpt_true)\n",
    "serving_acc = tf.saved_model.utils.build_tensor_info(ckpt_acc)\n",
    "serving_loss = tf.saved_model.utils.build_tensor_info(ckpt_loss)\n",
    "serving_pred = tf.saved_model.utils.build_tensor_info(ckpt_out)\n",
    "\n",
    "\"\"\" Define signature_definition: define input and output proto \"\"\"\n",
    "# a signature is the set of inputs to and outputs from a graph\n",
    "signature_definition = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={\n",
    "        'input_x': serving_input,\n",
    "    },\n",
    "    outputs={\n",
    "        'softmaxOut': serving_pred\n",
    "    },\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME \n",
    ")\n",
    "\n",
    "### export ###\n",
    "with ckpt_graph.as_default():\n",
    "    serving_builder.add_meta_graph_and_variables(ckpt_sess,\n",
    "                                                 [tf.saved_model.tag_constants.SERVING],\n",
    "                                                 signature_def_map={\n",
    "                                                     tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_definition\n",
    "                                                 })\n",
    "    serving_builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's check the exported model by **saved_model_cli** to make sure everything is set. (Especially for **method_name**, **tag**, and **signature key**)\n",
    "\n",
    "```shell\n",
    "#Terminal\n",
    "saved_model_cli show --dir Saved_Model/1 --all\n",
    "```\n",
    "\n",
    "![serve_model_cli](./pics/serve_model_cli.png)\n",
    "\n",
    "\n",
    "The model looks just fine! We can go for the server part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Serving with Docker\n",
    "\n",
    "Here, we‚Äôll start to setup our server with Docker. (If you forget this part, you can check [previous tutorial](https://paper.dropbox.com/doc/Deployment-Introduction--AgMfk42UNCSVDYgtk2pHmqKdAg-TrYTDU739OGQAP6iyidcc) or the [official document](https://www.tensorflow.org/tfx/serving/docker) for recap)\n",
    "\n",
    "Please run below command in your terminal.\n",
    "\n",
    "### a. Pull tensorflow/serving image from Docker Hub:\n",
    "```shell\n",
    "#terminal\n",
    "### pull tensorflow/serving image from Docker Hub ###\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "### check images in your local registry ###\n",
    "docker images\n",
    "```\n",
    "\n",
    "After pulling, we should see the tensorflow/serving image in our local registry now.\n",
    "\n",
    "![pull_image](./pics/pull_image.png)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### b. Run serving container via tensorflow/serving image:\n",
    "\n",
    "```shell\n",
    "#Terminal\n",
    "### build docker container ###\n",
    "\n",
    "docker run -p 8501:8501 -p 8500:8500 --mount type=bind,source=\"$(pwd)\"/Saved_Model,target=/models/myExcellentModel -e MODEL_NAME=myExcellentModel -it -d tensorflow/serving\n",
    "\n",
    "### check running container ###\n",
    "docker ps\n",
    "\n",
    "### show log of container in real-time ###\n",
    "docker logs -f <YOUR_CONTAINER_ID>\n",
    "```\n",
    "\n",
    "![run_container](./pics/run_container.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client : RESTfull API\n",
    "\n",
    "One of the major method for client to communicate with TensorFlow Serving server is through RESTfull API.\n",
    "\n",
    "\n",
    "We can get the inference requests via RESTfull API. \n",
    "\n",
    "First, let‚Äôs check our serving model‚Äôs metadata and status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Checking model metadata and model status:\n",
    "\n",
    "With model's metadata and status, we can know the information like input data shape, output shape, or the status of current serving model (AVAILABLE or END)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata:\n",
      "{\n",
      "\"model_spec\":{\n",
      " \"name\": \"myExcellentModel\",\n",
      " \"signature_name\": \"\",\n",
      " \"version\": \"1\"\n",
      "}\n",
      ",\n",
      "\"metadata\": {\"signature_def\": {\n",
      " \"signature_def\": {\n",
      "  \"serving_default\": {\n",
      "   \"inputs\": {\n",
      "    \"input_x\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"1\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"input/x_input:0\"\n",
      "    }\n",
      "   },\n",
      "   \"outputs\": {\n",
      "    \"softmaxOut\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"10\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"output_layer/Softmax:0\"\n",
      "    }\n",
      "   },\n",
      "   \"method_name\": \"tensorflow/serving/predict\"\n",
      "  }\n",
      " }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "status:\n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import necessary libraries\"\"\"\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\"\"\"checking model metadata\"\"\"\n",
    "print('metadata:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel/metadata').text)\n",
    "\n",
    "\"\"\"checking model status\"\"\"\n",
    "print('status:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Make predict request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [[0.124829, 0.0614052, 0.0995001, 0.157356, 0.0761166, 0.0929893, 0.0945179, 0.0863661, 0.113767, 0.0931527], [0.116204, 0.0558889, 0.107396, 0.144861, 0.0811449, 0.0963742, 0.0748477, 0.0818477, 0.130106, 0.111329], [0.132955, 0.0726666, 0.157384, 0.102203, 0.0766418, 0.0852655, 0.0950602, 0.0959828, 0.102379, 0.0794623]\n",
      "    ]\n",
      "} \n",
      "\n",
      "predictions: [3 3 2]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"preparing inference data\"\"\"\n",
    "payload = {'signature_name': 'serving_default', 'instances': x_batch[0:3].tolist()}\n",
    "data = json.dumps(payload)\n",
    "\n",
    "\"\"\"make request to the server\"\"\"\n",
    "headers = {'content-type': 'application/json'} #you can omit the header\n",
    "json_response = requests.post('http://localhost:8501/v1/models/myExcellentModel:predict', data=data, headers=headers)\n",
    "print(json_response.text, '\\n')\n",
    "\n",
    "\"\"\"parse prediction from response\"\"\"\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print('predictions: {}'.format(np.argmax(predictions, axis=-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output format is in accordance with the format mentioned in the [official document](https://www.tensorflow.org/tfx/serving/api_rest#response_format_4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client: gRPC\n",
    "\n",
    "Another method for client to communicate with TensorFlow Serving server is through gRPC API.\n",
    "\n",
    "We can do almost the same thing that RESTfull API can do through gRPC API.\n",
    "\n",
    "First, let‚Äôs check our serving model‚Äôs metadata and status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Checking model status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_version_status {\n",
      "  version: 1\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries\"\"\"\n",
    "import grpc\n",
    "from tensorflow_serving.apis import model_service_pb2_grpc\n",
    "from tensorflow_serving.apis import get_model_status_pb2\n",
    "\n",
    "\"\"\" create stub(client) via hosted port\"\"\"\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "status_stub = model_service_pb2_grpc.ModelServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request\"\"\"\n",
    "request = get_model_status_pb2.GetModelStatusRequest()\n",
    "request.model_spec.name = 'myExcellentModel'\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = status_stub.GetModelStatus(request, 2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Checking model metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_spec {\n",
      "  name: \"myExcellentModel\"\n",
      "  version {\n",
      "    value: 1\n",
      "  }\n",
      "}\n",
      "metadata {\n",
      "  key: \"signature_def\"\n",
      "  value {\n",
      "    type_url: \"type.googleapis.com/tensorflow.serving.SignatureDefMap\"\n",
      "    value: \"\\n\\250\\001\\n\\017serving_default\\022\\224\\001\\n9\\n\\007input_x\\022.\\n\\017input/x_input:0\\020\\001\\032\\031\\022\\013\\010\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\034\\022\\002\\010\\034\\022\\002\\010\\001\\022;\\n\\nsoftmaxOut\\022-\\n\\026output_layer/Softmax:0\\020\\001\\032\\021\\022\\013\\010\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\n\\032\\032tensorflow/serving/predict\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries \"\"\"\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "from tensorflow_serving.apis import get_model_metadata_pb2\n",
    "\n",
    "\"\"\" create stub (client) via hosted port \"\"\"\n",
    "meta_stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request \"\"\"\n",
    "request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "request.model_spec.name = 'myExcellentModel'\n",
    "request.metadata_field.append('signature_def')\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = meta_stub.GetModelMetadata(request, 2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Make predict request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries \"\"\"\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "\n",
    "\"\"\" create stub (client) via hosted port \"\"\" \n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request \"\"\"\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'myExcellentModel'\n",
    "request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY #serving_default\n",
    "request.inputs['input_x'].CopyFrom(tf.make_tensor_proto(x_batch[0:3].astype(dtype=np.float32), shape=[3, 28, 28, 1])) #you can omit shape here\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = stub.Predict(request, 2) # Synchronous request\n",
    "# response = stub.Predict.future(request, 2) #Asynchronous request\n",
    "\n",
    "\"\"\" get the output scores \"\"\"\n",
    "result = response.outputs['softmaxOut'].float_val\n",
    "print([np.argmax(result[i*10:(i+1)*10]) for i in range(len(x_batch[0:3]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New version in da house:\n",
    "\n",
    "Let‚Äôs say our customers ask us to give them a service with higher accuracy. Now, we need to train a new model and deploy it without shutting down current service. \n",
    "\n",
    "Now, just keep your server running, and start training a new model and export new model for serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Train a New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 11:51:02.296238 4541388224 deprecation.py:323] From <ipython-input-138-37e4ca855b92>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0702 11:51:02.395655 4541388224 deprecation.py:506] From /Users/admin/Documents/Cinnamon/Bootcamp/Serving_Example/serve_env/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0702 11:51:02.938024 4541388224 deprecation.py:323] From <ipython-input-138-37e4ca855b92>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0702 11:51:03.412230 4541388224 deprecation.py:323] From <ipython-input-138-37e4ca855b92>:43: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0702 11:51:04.015784 4541388224 deprecation.py:323] From <ipython-input-138-37e4ca855b92>:57: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Define model Graph\n",
    "#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "main_graph = tf.Graph()\n",
    "\n",
    "with main_graph.as_default():\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        inputs = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                                None, 28, 28, 1], name='x_input')\n",
    "        y_true = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                                None, 10], name='y_true')\n",
    "        lr = tf.placeholder(dtype=tf.float32, shape=None, name='learning_rate')\n",
    "\n",
    "    with tf.variable_scope('hidden_layers'):\n",
    "        model = tf.layers.conv2d(inputs=inputs,\n",
    "                                 filters=16,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 strides=(1, 1),\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 )\n",
    "        model = tf.layers.max_pooling2d(inputs=model,\n",
    "                                        pool_size=(2, 2),\n",
    "                                        strides=2,\n",
    "                                        )\n",
    "        model = tf.layers.conv2d(inputs=inputs,\n",
    "                                 filters=16,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 strides=(1, 1),\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 )\n",
    "        model = tf.layers.max_pooling2d(inputs=model,\n",
    "                                        pool_size=(2, 2),\n",
    "                                        strides=2,\n",
    "                                        )\n",
    "\n",
    "    with tf.variable_scope('output_layer'):\n",
    "        model = tf.layers.Flatten()(model)\n",
    "        output = tf.layers.dense(inputs=model, units=10)\n",
    "        prediction = tf.nn.softmax(output)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output,\n",
    "                                                                         labels=y_true,\n",
    "                                                                         name='cross_entropy_loss'))\n",
    "\n",
    "        optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        gra_and_var = optim.compute_gradients(loss)\n",
    "        update = optim.apply_gradients(gra_and_var)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_predict = tf.equal(tf.arg_max(\n",
    "            prediction, dimension=1), tf.arg_max(y_true, 1))\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predict, dtype=tf.float32), name='accuracy')\n",
    "\n",
    "    init_main_graph = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyperparameters\n",
    "#\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 10000\n",
    "total_batch = int(len(x_train) / batch_size)\n",
    "reduceLR = ReduceLROnPlateau(lr=0.001, factor=0.5, patience=10)\n",
    "train_loss, train_acc = [], []\n",
    "valid_loss, valid_acc = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1: tr_loss 2.164, tr_acc 0.319; val_loss 1.964, val_acc 0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2: tr_loss 1.829, tr_acc 0.647; val_loss 1.619, val_acc 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "\n",
    "\n",
    "sess = tf.Session(graph=main_graph)\n",
    "sess.run(init_main_graph)\n",
    "\n",
    "for epc in tqdm.tqdm(range(epochs)):\n",
    "    tr_loss_tmp, tr_acc_tmp = 0, 0\n",
    "    val_loss_tmp, val_acc_tmp = 0, 0\n",
    "    train_gen = trainData_generator(x_train, y_train, total_batch=total_batch)\n",
    "\n",
    "    ### training ###\n",
    "    for x_batch, y_batch in train_gen:\n",
    "\n",
    "        tr_acc_batch, tr_loss_batch, _ = sess.run([accuracy, loss, update], feed_dict={\n",
    "            inputs: x_batch,\n",
    "            y_true: y_batch,\n",
    "            lr: reduceLR.lr\n",
    "        })\n",
    "\n",
    "        tr_loss_tmp += tr_loss_batch\n",
    "        tr_acc_tmp += tr_acc_batch\n",
    "\n",
    "    train_loss.append(tr_loss_tmp / total_batch)\n",
    "    train_acc.append(tr_acc_tmp / total_batch)\n",
    "\n",
    "    ### validation ###\n",
    "    x_batch, y_batch = validData_generator(x_test, y_test)\n",
    "\n",
    "    val_acc_batch, val_loss_batch = sess.run([accuracy, loss], feed_dict={\n",
    "        inputs: x_batch,\n",
    "        y_true: y_batch\n",
    "    })\n",
    "\n",
    "    val_loss_tmp += val_loss_batch\n",
    "    val_acc_tmp += val_acc_batch\n",
    "\n",
    "    valid_loss.append(val_loss_tmp)\n",
    "    valid_acc.append(val_acc_tmp)\n",
    "    reduceLR.on_epoch_end(valid_loss[-1])\n",
    "\n",
    "    print(' Epoch {}: tr_loss {:.3f}, tr_acc {:.3f}; val_loss {:.3f}, val_acc {:.3f}'.format(epc+1,\n",
    "                                                                                             train_loss[-1],\n",
    "                                                                                             train_acc[-1],\n",
    "                                                                                             valid_loss[-1],\n",
    "                                                                                             valid_acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Export New Model for Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get tensors and check accuracy and loss\n",
    "#\n",
    "\n",
    "ckpt_acc = sess.graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "ckpt_loss = sess.graph.get_tensor_by_name('loss/Mean:0')\n",
    "ckpt_input = sess.graph.get_tensor_by_name('input/x_input:0')\n",
    "ckpt_true = sess.graph.get_tensor_by_name('input/y_true:0')\n",
    "ckpt_out = sess.graph.get_tensor_by_name('output_layer/Softmax:0')\n",
    "\n",
    "\n",
    "#\n",
    "# Export Model for serving\n",
    "#\n",
    "export_path = './Saved_Model/2'\n",
    "serving_builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "### Get tensor info from input and output tensors: transform tensor to TensorInfo protocol buff ###\n",
    "serving_input = tf.saved_model.utils.build_tensor_info(ckpt_input)\n",
    "serving_label = tf.saved_model.utils.build_tensor_info(ckpt_true)\n",
    "serving_acc = tf.saved_model.utils.build_tensor_info(ckpt_acc)\n",
    "serving_loss = tf.saved_model.utils.build_tensor_info(ckpt_loss)\n",
    "serving_pred = tf.saved_model.utils.build_tensor_info(ckpt_out)\n",
    "\n",
    "### Define signature_definition: define input and output proto ###\n",
    "# a signature is the set of inputs to and outputs from a graph\n",
    "signature_definition = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={\n",
    "        'input_x': serving_input,\n",
    "    },\n",
    "    outputs={\n",
    "        'softmaxOut': serving_pred\n",
    "    },\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "\n",
    "### export ###\n",
    "with main_graph.as_default():\n",
    "    serving_builder.add_meta_graph_and_variables(sess,\n",
    "                                                 [tf.saved_model.tag_constants.SERVING],\n",
    "                                                 signature_def_map={\n",
    "                                                     tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_definition\n",
    "                                                 })\n",
    "    serving_builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see the server loading version 2 and unloading version 1.\n",
    "\n",
    "![new_model_loading](./pics/new_model_loading.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Serving with Model Config:\n",
    "\n",
    "In some circumstances, we might want to serve multiple models with multiple serving policy.\n",
    "\n",
    "We can achieve this by model config file.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### a. Fake Some Models:\n",
    "\n",
    "First of all, let‚Äôs fake some servable models just by copy-paste the exported model.\n",
    "\n",
    "```shell\n",
    "\n",
    "    #Terminal\n",
    "    ### create a folder called Saved_Models ###\n",
    "    mkdir Saved_Models\n",
    "    cd Saved_Models\n",
    "    \n",
    "    ### make three folders for storing three servable models ###\n",
    "    mkdir myExcellentModel myExcellentModel2 myExcellentModel3\n",
    "    \n",
    "    ### copy the exported model in Saved_Model to the three folders ###\n",
    "    cp -r ../Saved_Model/1 myExcellentModel/1\n",
    "    cp -r ../Saved_Model/2 myExcellentModel/2\n",
    "    cp -r ../Saved_Model/1 myExcellentModel2/1\n",
    "    cp -r ../Saved_Model/1 myExcellentModel2/2\n",
    "    cp -r ../Saved_Model/1 myExcellentModel3/1\n",
    "    cp -r ../Saved_Model/1 myExcellentModel3/2\n",
    "```\n",
    "\n",
    "\n",
    "Now, the Saved_Models folder should look like below.\n",
    "\n",
    "![saved_models_folder](./pics/saved_models_folder.png)\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### b. Write Model Config File:\n",
    "\n",
    "Here comes the fun part. Let‚Äôs write the model config file serving_model.conf.\n",
    "\n",
    "```shell\n",
    "#serving_model.conf\n",
    "model_config_list: {\n",
    "    config: {\n",
    "        name: \"myExcellentModel\",\n",
    "        base_path: \"/models/myExcellentModel\",\n",
    "        model_platform: \"tensorflow\",\n",
    "        model_version_policy: {\n",
    "            all{}\n",
    "        }\n",
    "    }\n",
    "    config: {\n",
    "        name: \"myExcellentModel2\",\n",
    "        base_path: \"/models/myExcellentModel2\",\n",
    "        model_platform: \"tensorflow\",\n",
    "        model_version_policy: {latest{\n",
    "            num_versions: 1 \n",
    "        }}\n",
    "    }\n",
    "    config: {\n",
    "        name: \"myExcellentModel3\",\n",
    "        base_path: \"/models/myExcellentModel3\",\n",
    "        model_platform: \"tensorflow\",\n",
    "        model_version_policy: {specific{\n",
    "            versions: 1,\n",
    "            versions: 2\n",
    "        }}\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We assign three difference model_version_policy to these three different models. Now, let‚Äôs see the differences.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### c. Run TensorFlow Serving with Model Config File:\n",
    "```shell\n",
    "#Terminal\n",
    "### Run TensorFlow Serving ###\n",
    "docker run -p 8501:8501 -p 8500:8500 \\\n",
    "    --mount type=bind,source=\"$(pwd)\"/Saved_Models,target=/models/ \\\n",
    "    --mount type=bind,source=\"$(pwd)\"/serving_model.conf,target=/models/model.conf \\\n",
    "    -it -d tensorflow/serving --model_config_file=/models/model.conf\n",
    "\n",
    "### check container ID ###\n",
    "docker ps\n",
    "\n",
    "### show log of container in real-time ###\n",
    "docker logs -f <YOUR_CONTAINER_ID>\n",
    "```\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Here are couple of things we need to be aware of.\n",
    "\n",
    "\n",
    "- We **bind Saved_Models folder to container‚Äôs models folder**. (Just like what we did in the previous example)\n",
    "\n",
    "\n",
    "- We also **bind serving_model.conf file to container‚Äôs /models/model.conf**. (Here is where the TensorFlow Serving will look for config file)\n",
    "\n",
    "\n",
    "- We **don‚Äôt need to specify the MODEL_NAME and MODEL_BASE_PATH here**. (We already specify them in the model config file)\n",
    "\n",
    "\n",
    "After that, we will see a bunch of log messages.  Check the log messages, we should see something like below.\n",
    "\n",
    "\n",
    "![model_config_serving_1](./pics/model_config_serving_1.png)\n",
    "![model_config_serving_2](./pics/model_config_serving_2.png)\n",
    "![model_config_serving_3](./pics/model_config_serving_3.png)\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The server loads all the models according to the version policy that we write in the model config file. (ex: It loads the version 1 and 2 of myExcellentModel3 because we specify the specific policy in the model config file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Check models' metadata: RESTfull API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myExcellentModel metadata:\n",
      "{\n",
      "\"model_spec\":{\n",
      " \"name\": \"myExcellentModel\",\n",
      " \"signature_name\": \"\",\n",
      " \"version\": \"2\"\n",
      "}\n",
      ",\n",
      "\"metadata\": {\"signature_def\": {\n",
      " \"signature_def\": {\n",
      "  \"serving_default\": {\n",
      "   \"inputs\": {\n",
      "    \"input_x\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"1\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"input/x_input:0\"\n",
      "    }\n",
      "   },\n",
      "   \"outputs\": {\n",
      "    \"softmaxOut\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"10\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"output_layer/Softmax:0\"\n",
      "    }\n",
      "   },\n",
      "   \"method_name\": \"tensorflow/serving/predict\"\n",
      "  }\n",
      " }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "myExcellentModel:version 1 metadata\n",
      "{\n",
      "\"model_spec\":{\n",
      " \"name\": \"myExcellentModel\",\n",
      " \"signature_name\": \"\",\n",
      " \"version\": \"1\"\n",
      "}\n",
      ",\n",
      "\"metadata\": {\"signature_def\": {\n",
      " \"signature_def\": {\n",
      "  \"serving_default\": {\n",
      "   \"inputs\": {\n",
      "    \"input_x\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"1\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"input/x_input:0\"\n",
      "    }\n",
      "   },\n",
      "   \"outputs\": {\n",
      "    \"softmaxOut\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"10\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"output_layer/Softmax:0\"\n",
      "    }\n",
      "   },\n",
      "   \"method_name\": \"tensorflow/serving/predict\"\n",
      "  }\n",
      " }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "myExcellentModel2 metadata:\n",
      "{\n",
      "\"model_spec\":{\n",
      " \"name\": \"myExcellentModel2\",\n",
      " \"signature_name\": \"\",\n",
      " \"version\": \"2\"\n",
      "}\n",
      ",\n",
      "\"metadata\": {\"signature_def\": {\n",
      " \"signature_def\": {\n",
      "  \"serving_default\": {\n",
      "   \"inputs\": {\n",
      "    \"input_x\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"1\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"input/x_input:0\"\n",
      "    }\n",
      "   },\n",
      "   \"outputs\": {\n",
      "    \"softmaxOut\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"10\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"output_layer/Softmax:0\"\n",
      "    }\n",
      "   },\n",
      "   \"method_name\": \"tensorflow/serving/predict\"\n",
      "  }\n",
      " }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "myExcellentModel3 metadata:\n",
      "{\n",
      "\"model_spec\":{\n",
      " \"name\": \"myExcellentModel3\",\n",
      " \"signature_name\": \"\",\n",
      " \"version\": \"2\"\n",
      "}\n",
      ",\n",
      "\"metadata\": {\"signature_def\": {\n",
      " \"signature_def\": {\n",
      "  \"serving_default\": {\n",
      "   \"inputs\": {\n",
      "    \"input_x\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"1\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"input/x_input:0\"\n",
      "    }\n",
      "   },\n",
      "   \"outputs\": {\n",
      "    \"softmaxOut\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"10\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"output_layer/Softmax:0\"\n",
      "    }\n",
      "   },\n",
      "   \"method_name\": \"tensorflow/serving/predict\"\n",
      "  }\n",
      " }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "myExcellentModel3:version 1 metadata\n",
      "{\n",
      "\"model_spec\":{\n",
      " \"name\": \"myExcellentModel3\",\n",
      " \"signature_name\": \"\",\n",
      " \"version\": \"1\"\n",
      "}\n",
      ",\n",
      "\"metadata\": {\"signature_def\": {\n",
      " \"signature_def\": {\n",
      "  \"serving_default\": {\n",
      "   \"inputs\": {\n",
      "    \"input_x\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"28\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"1\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"input/x_input:0\"\n",
      "    }\n",
      "   },\n",
      "   \"outputs\": {\n",
      "    \"softmaxOut\": {\n",
      "     \"dtype\": \"DT_FLOAT\",\n",
      "     \"tensor_shape\": {\n",
      "      \"dim\": [\n",
      "       {\n",
      "        \"size\": \"-1\",\n",
      "        \"name\": \"\"\n",
      "       },\n",
      "       {\n",
      "        \"size\": \"10\",\n",
      "        \"name\": \"\"\n",
      "       }\n",
      "      ],\n",
      "      \"unknown_rank\": false\n",
      "     },\n",
      "     \"name\": \"output_layer/Softmax:0\"\n",
      "    }\n",
      "   },\n",
      "   \"method_name\": \"tensorflow/serving/predict\"\n",
      "  }\n",
      " }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import necessary libraries\"\"\"\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\"\"\"checking model metadata\"\"\"\n",
    "print('myExcellentModel metadata:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel/metadata').text)\n",
    "print('myExcellentModel:version 1 metadata')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel/versions/1/metadata').text)\n",
    "\n",
    "print('myExcellentModel2 metadata:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel2/metadata').text)\n",
    "\n",
    "print('myExcellentModel3 metadata:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel3/metadata').text)\n",
    "print('myExcellentModel3:version 1 metadata')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel3/versions/1/metadata').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Check models' status: RESTfull API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myExcellentModel status:\n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"2\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  },\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n",
      "myExcellentModel2 status:\n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"3\",\n",
      "   \"state\": \"END\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  },\n",
      "  {\n",
      "   \"version\": \"2\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n",
      "myExcellentModel3 status:\n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"2\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  },\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"checking model status\"\"\"\n",
    "print('myExcellentModel status:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel').text)\n",
    "\n",
    "print('myExcellentModel2 status:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel2').text)\n",
    "\n",
    "print('myExcellentModel3 status:')\n",
    "print(requests.get('http://localhost:8501/v1/models/myExcellentModel3').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Make predict request: RESTfull API (Take myExcellentModel version 2 as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [[0.103439, 0.0853091, 0.0982532, 0.118523, 0.0811818, 0.0819828, 0.120437, 0.0812559, 0.137039, 0.0925796], [0.0946726, 0.0570425, 0.133527, 0.105784, 0.114615, 0.0966139, 0.0889287, 0.106509, 0.0995103, 0.102796], [0.186527, 0.0494206, 0.0914837, 0.116658, 0.080768, 0.0927301, 0.100025, 0.0688501, 0.118372, 0.0951648]\n",
      "    ]\n",
      "} \n",
      "\n",
      "predictions: [8 2 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"preparing inference data\"\"\"\n",
    "payload = {'signature_name': 'serving_default', 'instances': x_batch[0:3].tolist()}\n",
    "data = json.dumps(payload)\n",
    "\n",
    "\"\"\"make request to the server\"\"\"\n",
    "headers = {'content-type': 'application/json'} #you can omit the header\n",
    "json_response = requests.post('http://localhost:8501/v1/models/myExcellentModel/versions/2:predict', data=data, headers=headers)\n",
    "print(json_response.text, '\\n')\n",
    "\n",
    "\"\"\"parse prediction from response\"\"\"\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print('predictions: {}'.format(np.argmax(predictions, axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Check model status: gRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myExcellentModel:\n",
      "model_version_status {\n",
      "  version: 2\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "model_version_status {\n",
      "  version: 1\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n",
      "myExcellentModel2:\n",
      "model_version_status {\n",
      "  version: 3\n",
      "  state: END\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "model_version_status {\n",
      "  version: 2\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n",
      "myExcellentModel3:\n",
      "model_version_status {\n",
      "  version: 1\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries\"\"\"\n",
    "import grpc\n",
    "from tensorflow_serving.apis import model_service_pb2_grpc\n",
    "from tensorflow_serving.apis import get_model_status_pb2\n",
    "\n",
    "\"\"\" create stub(client) via hosted port\"\"\"\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "status_stub = model_service_pb2_grpc.ModelServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request\"\"\"\n",
    "request = get_model_status_pb2.GetModelStatusRequest()\n",
    "request.model_spec.name = 'myExcellentModel'\n",
    "\n",
    "request2 = get_model_status_pb2.GetModelStatusRequest()\n",
    "request2.model_spec.name = 'myExcellentModel2'\n",
    "\n",
    "request3 = get_model_status_pb2.GetModelStatusRequest()\n",
    "request3.model_spec.name = 'myExcellentModel3'\n",
    "request3.model_spec.version.value = 1 #assign version value\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = status_stub.GetModelStatus(request, 2)\n",
    "print('myExcellentModel:')\n",
    "print(response)\n",
    "\n",
    "response2 = status_stub.GetModelStatus(request2, 2)\n",
    "print('myExcellentModel2:')\n",
    "print(response2)\n",
    "\n",
    "response3 = status_stub.GetModelStatus(request3, 2)\n",
    "print('myExcellentModel3:')\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. Check model metadata: gRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_spec {\n",
      "  name: \"myExcellentModel3\"\n",
      "  version {\n",
      "    value: 1\n",
      "  }\n",
      "}\n",
      "metadata {\n",
      "  key: \"signature_def\"\n",
      "  value {\n",
      "    type_url: \"type.googleapis.com/tensorflow.serving.SignatureDefMap\"\n",
      "    value: \"\\n\\250\\001\\n\\017serving_default\\022\\224\\001\\n9\\n\\007input_x\\022.\\n\\017input/x_input:0\\020\\001\\032\\031\\022\\013\\010\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\034\\022\\002\\010\\034\\022\\002\\010\\001\\022;\\n\\nsoftmaxOut\\022-\\n\\026output_layer/Softmax:0\\020\\001\\032\\021\\022\\013\\010\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\n\\032\\032tensorflow/serving/predict\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Here we only take myExcellentModel version 1 as an example ###\n",
    "\n",
    "\"\"\" import necessary libraries \"\"\"\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "from tensorflow_serving.apis import get_model_metadata_pb2\n",
    "\n",
    "\"\"\" create stub (client) via hosted port \"\"\"\n",
    "meta_stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request \"\"\"\n",
    "request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "request.model_spec.name = 'myExcellentModel3'\n",
    "request.model_spec.version.value = 1\n",
    "request.metadata_field.append('signature_def')\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = meta_stub.GetModelMetadata(request, 2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Make predict request: gRPC (Take myExcellentModel version 1 as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries \"\"\"\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "\n",
    "\"\"\" create stub (client) via hosted port \"\"\" \n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request \"\"\"\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'myExcellentModel'\n",
    "request.model_spec.version.value = 1\n",
    "request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY #serving_default\n",
    "request.inputs['input_x'].CopyFrom(tf.make_tensor_proto(x_batch[0:3].astype(dtype=np.float32)))\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = stub.Predict(request, 2) # Synchronous request\n",
    "# response = stub.Predict.future(request, 2) #Asynchronous request\n",
    "\n",
    "\"\"\" get the output scores \"\"\"\n",
    "result = response.outputs['softmaxOut'].float_val\n",
    "print([np.argmax(result[i*10:(i+1)*10]) for i in range(len(x_batch[0:3]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Model Config File at Runtime\n",
    "\n",
    "Now, let‚Äôs say we have new strategy to server our fabulous models. We don‚Äôt want our customer to use all version of our models, and we don‚Äôt want them to know which version they are using. How can we do all the above changes with out shutting down our service? We can do that through gRPC.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "### a. Create a new model config file called serving_model_new.conf \n",
    "\n",
    "```shell\n",
    "#terminal\n",
    "vim serving_model_new.conf\n",
    "\n",
    "#copy and paste below new config file\n",
    "model_config_list: {\n",
    "    config: {\n",
    "        name: \"myExcellentModel\",\n",
    "        base_path: \"/models/myExcellentModel\",\n",
    "        model_platform: \"tensorflow\",\n",
    "        model_version_policy: {latest{\n",
    "            num_versions: 1\n",
    "        }}\n",
    "    }\n",
    "    config: {\n",
    "        name: \"myExcellentModel2\",\n",
    "        base_path: \"/models/myExcellentModel2\",\n",
    "        model_platform: \"tensorflow\",\n",
    "        model_version_policy: {latest{\n",
    "            num_versions: 1                                         \n",
    "        }}\n",
    "    }\n",
    "    config: {\n",
    "        name: \"myExcellentModel3\",\n",
    "        base_path: \"/models/myExcellentModel3\",\n",
    "        model_platform: \"tensorflow\",\n",
    "        model_version_policy: {specific{\n",
    "            versions: 2,\n",
    "            versions: 1                                                    \n",
    "        }}\n",
    "        version_labels: {\n",
    "            key: 'stable'\n",
    "            value: 1 \n",
    "        }\n",
    "        version_labels: {\n",
    "            key: 'canary'\n",
    "            value: 2 \n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Here we change the version policy of myExcellentModel and myExcellentModel2. Also, we assign **version labels** to myExcellentModel3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Update server‚Äôs model config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/admin/Documents/Cinnamon/Bootcamp/Serving_Example'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[(<google.protobuf.pyext._message.FieldDescriptor object at 0x147ab3dd8>, model_config_list {\n",
      "  config {\n",
      "    name: \"myExcellentModel\"\n",
      "    base_path: \"/models/myExcellentModel\"\n",
      "    model_platform: \"tensorflow\"\n",
      "    model_version_policy {\n",
      "      latest {\n",
      "        num_versions: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  config {\n",
      "    name: \"myExcellentModel2\"\n",
      "    base_path: \"/models/myExcellentModel2\"\n",
      "    model_platform: \"tensorflow\"\n",
      "    model_version_policy {\n",
      "      latest {\n",
      "        num_versions: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  config {\n",
      "    name: \"myExcellentModel3\"\n",
      "    base_path: \"/models/myExcellentModel3\"\n",
      "    model_platform: \"tensorflow\"\n",
      "    model_version_policy {\n",
      "      specific {\n",
      "        versions: 2\n",
      "        versions: 1\n",
      "      }\n",
      "    }\n",
      "    version_labels {\n",
      "      key: \"canary\"\n",
      "      value: 2\n",
      "    }\n",
      "    version_labels {\n",
      "      key: \"stable\"\n",
      "      value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      ")]\n",
      "Reload sucessfully\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_serving.apis import model_service_pb2_grpc\n",
    "from tensorflow_serving.apis import model_management_pb2\n",
    "from tensorflow_serving.config import model_server_config_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "import grpc\n",
    "\n",
    "def update_model_config(host, names, base_path, model_platform, config_file_path):\n",
    "    '''\n",
    "    Function to update model config list at serving runtime\n",
    "    Args:\n",
    "        host: str, hosting port\n",
    "        names: list, list of models' names\n",
    "        base_path: str, model's base path\n",
    "        model_platform: str, model's platform, ex: tensorflow\n",
    "        config_file_path: str, the path of the new model config file\n",
    "    '''\n",
    "    \n",
    "    \"\"\" read the config file\"\"\"\n",
    "    with open(config_file_path, 'r+') as f:\n",
    "        config_ini = f.read()\n",
    "    \n",
    "    \"\"\" parse text and merge into model server config\"\"\"\n",
    "    model_server_config = model_server_config_pb2.ModelServerConfig()\n",
    "    model_server_config = text_format.Parse(text=config_ini, message=model_server_config)\n",
    "    \n",
    "    \n",
    "    \"\"\" create stub(client) via hosted port\"\"\"\n",
    "    channel = grpc.insecure_channel(host) \n",
    "    stub = model_service_pb2_grpc.ModelServiceStub(channel)\n",
    "    \n",
    "    \"\"\" create request \"\"\"\n",
    "    request = model_management_pb2.ReloadConfigRequest() \n",
    "\n",
    "    \"\"\" create a config to add to the list of served models\"\"\"\n",
    "    config_list = model_server_config_pb2.ModelConfigList()\n",
    "    \n",
    "    \"\"\" config request\"\"\"\n",
    "    request.config.CopyFrom(model_server_config)\n",
    "\n",
    "    print(request.IsInitialized())\n",
    "    print(request.ListFields())\n",
    "    \n",
    "    \"\"\" handle reload request\"\"\"\n",
    "    response = stub.HandleReloadConfigRequest(request,10)\n",
    "    \n",
    "    if response.status.error_code == 0:\n",
    "        print(\"Reload sucessfully\")\n",
    "    else:\n",
    "        print(\"Reload failed!\")\n",
    "        print(response.status.error_code)\n",
    "        print(response.status.error_message)\n",
    "\n",
    "\n",
    "update_model_config(host=\"localhost:8500\", \n",
    "                    names=[\"myExcellentModel\", \"myExcellentModel2\", \"myExcellentModel3\"], \n",
    "                    base_path=\"/models\", \n",
    "                    model_platform=\"tensorflow\", \n",
    "                    config_file_path='./serving_model_new.conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Check models' status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myExcellentModel:\n",
      "model_version_status {\n",
      "  version: 2\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "model_version_status {\n",
      "  version: 1\n",
      "  state: END\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n",
      "myExcellentModel2:\n",
      "model_version_status {\n",
      "  version: 3\n",
      "  state: END\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "model_version_status {\n",
      "  version: 2\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n",
      "myExcellentModel3:\n",
      "model_version_status {\n",
      "  version: 2\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "model_version_status {\n",
      "  version: 1\n",
      "  state: AVAILABLE\n",
      "  status {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries\"\"\"\n",
    "import grpc\n",
    "from tensorflow_serving.apis import model_service_pb2_grpc\n",
    "from tensorflow_serving.apis import get_model_status_pb2\n",
    "\n",
    "\"\"\" create stub(client) via hosted port\"\"\"\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "status_stub = model_service_pb2_grpc.ModelServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request\"\"\"\n",
    "request = get_model_status_pb2.GetModelStatusRequest()\n",
    "request.model_spec.name = 'myExcellentModel'\n",
    "\n",
    "request2 = get_model_status_pb2.GetModelStatusRequest()\n",
    "request2.model_spec.name = 'myExcellentModel2'\n",
    "\n",
    "request3 = get_model_status_pb2.GetModelStatusRequest()\n",
    "request3.model_spec.name = 'myExcellentModel3'\n",
    "\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = status_stub.GetModelStatus(request, 2)\n",
    "print('myExcellentModel:')\n",
    "print(response)\n",
    "\n",
    "response2 = status_stub.GetModelStatus(request2, 2)\n",
    "print('myExcellentModel2:')\n",
    "print(response2)\n",
    "\n",
    "response3 = status_stub.GetModelStatus(request3, 2)\n",
    "print('myExcellentModel3:')\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Make predict request via version label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import necessary libraries \"\"\"\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "\n",
    "\"\"\" create stub (client) via hosted port \"\"\" \n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "\"\"\" create and config request \"\"\"\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'myExcellentModel3'\n",
    "request.model_spec.version_label = 'stable' #assing version label\n",
    "request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY #serving_default\n",
    "request.inputs['input_x'].CopyFrom(tf.make_tensor_proto(x_batch[0:3].astype(dtype=np.float32)))\n",
    "\n",
    "\"\"\" send request and get response \"\"\"\n",
    "response = stub.Predict(request, 2) # Synchronous request\n",
    "# response = stub.Predict.future(request, 2) #Asynchronous request\n",
    "\n",
    "\"\"\" get the output scores \"\"\"\n",
    "result = response.outputs['softmaxOut'].float_val\n",
    "print([np.argmax(result[i*10:(i+1)*10]) for i in range(len(x_batch[0:3]))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serve_env",
   "language": "python",
   "name": "serve_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
