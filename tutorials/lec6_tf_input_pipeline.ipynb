{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "#img_path = \"/home/Data/CharactersTrimPad28/\"\n",
    "img_path = \"./s3mnt/ChineseNumbers/\"\n",
    "img_path = \"/home/Data/ChineseNumbers/\"\n",
    "data_root = pathlib.Path(img_path)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "  image = tf.image.decode_image(image, channels=3)\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255.0  # normalize to [0,1] range\n",
    "  return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "  image = tf.read_file(path)\n",
    "  return preprocess_image(image)\n",
    "\n",
    "# The tuples are unpacked into the positional arguments of the mapped function\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "  return load_and_preprocess_image(path), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Data/ChineseNumbers/二/Han yi Cu yuan ti Font-Traditional Chinese ttf.png\n",
      "/home/Data/ChineseNumbers/二/JiaShang Liu Xing kai 5500 Font- Simplified Chinesettf.png\n",
      "/home/Data/ChineseNumbers/二/Japan hengshan writing brush Font-Traditional Chinesettf.png\n",
      "/home/Data/ChineseNumbers/二/Classic Cu hei Fontttf.png\n",
      "/home/Data/ChineseNumbers/二/Chinese New Year(DFGirl-dospy-fei) font-Simplified Chinesettf.png\n",
      "/home/Data/ChineseNumbers/二/Han yi Fang die Fontttf.png\n",
      "/home/Data/ChineseNumbers/二/Classic Kong die hei Fontttf.png\n",
      "/home/Data/ChineseNumbers/二/Childhood amusement park Font-Simplified Chinesettf.png\n",
      "/home/Data/ChineseNumbers/二/Snow World  Butterfly Font-Simplified Chinesettf.png\n",
      "/home/Data/ChineseNumbers/二/Hypocrite Youth v 20 Font-Simplified ChineseTTF.png\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset.from_tensor_slices\n",
    "all_image_paths = [str(path) for path in list(data_root.glob('*/*'))]\n",
    "for i in range(10):\n",
    "    print(all_image_paths[i])\n",
    "\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "image_count = len(all_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test iterate time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(dataset):\n",
    "    overall_start = time.time()\n",
    "    n_image = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for n_batch,(images,labels) in enumerate(dataset):\n",
    "        n_image += int(images.shape[0])\n",
    "        if n_image%100 == 0:\n",
    "            #print('.',end='')\n",
    "            #print(\"\\r{} images in {} batches with BATCH_SIZE {}: {:.2f} s\".format(n_image, n_batch, BATCH_SIZE, time.time()-start), end='', flush=True)\n",
    "            print(\"\\r{} images: {:.2f} s\".format(n_image, time.time()-start), end='', flush=True)\n",
    "    print()\n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    \n",
    "    print(\"{} images: {} s\".format(n_image, duration))\n",
    "    print(\"{:0.5f} Images/s\".format(n_image/float(duration)))\n",
    "    print(\"Total time: {}s\".format(end-overall_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 3.40 s\n",
      "12607 images: 3.565103530883789 s\n",
      "3536.22269 Images/s\n",
      "Total time: 3.5651044845581055s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Map with num_parallel_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 04:53:54.214915 140646928393984 deprecation.py:323] From <ipython-input-7-c80bf333129f>:7: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 1.43 s\n",
      "12607 images: 1.497429370880127 s\n",
      "8419.09491 Images/s\n",
      "Total time: 1.4974303245544434s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tf.data.experimental.shuffle_and_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.experimental.shuffle_and_repeat](https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat)\n",
    "```\n",
    "tf.data.experimental.shuffle_and_repeat(\n",
    "    buffer_size,\n",
    "    count=None,\n",
    "    seed=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 1.37 s\n",
      "12607 images: 1.4445338249206543 s\n",
      "8727.38304 Images/s\n",
      "Total time: 1.4445347785949707s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. tf.data.experimental.map_and_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.experimental.map_and_batch](https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch)\n",
    "```\n",
    "tf.data.experimental.map_and_batch(\n",
    "    map_func,\n",
    "    batch_size,\n",
    "    num_parallel_batches=None,\n",
    "    drop_remainder=False,\n",
    "    num_parallel_calls=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 04:53:57.516138 140646928393984 deprecation.py:323] From <ipython-input-9-a731ffa1e19b>:8: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 1.38 s\n",
      "12607 images: 1.4397857189178467 s\n",
      "8756.16408 Images/s\n",
      "Total time: 1.439786434173584s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.apply(tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=4))\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. shuffle_and_repeat + map_and_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 1.40 s\n",
      "12607 images: 1.4625129699707031 s\n",
      "8620.09449 Images/s\n",
      "Total time: 1.4625139236450195s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "image_label_ds = path_label_ds.apply(tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=4))\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.experimental.prefetch_to_device](https://www.tensorflow.org/api_docs/python/tf/data/experimental/prefetch_to_device)\n",
    "```\n",
    "tf.data.experimental.prefetch_to_device(\n",
    "    device,\n",
    "    buffer_size=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 1.39 s\n",
      "12607 images: 1.4531762599945068 s\n",
      "8675.47891 Images/s\n",
      "Total time: 1.4531772136688232s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "image_label_ds = image_label_ds.prefetch(buffer_size=1) # Only on CPU\n",
    "#image_label_ds = image_label_ds.apply(tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=1)) # Must be final Dataset in input pipeline\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.Dataset.cache](https://www.tensorflow.org/tutorials/load_data/images#cache)\n",
    "                                       \n",
    "Use tf.data.Dataset.cache to easily cache calculations across epochs. This is especially performant if the dataq fits in memory\n",
    "```\n",
    "ds = image_label_ds.cache()\n",
    "```\n",
    "\n",
    "One disadvantage to using an in memory cache is that the cache must be rebuilt on each run, giving the same startup delay each time the dataset is started:\n",
    "If the data doesn't fit in memory, use a cache file. \n",
    "The cache file also has the advantage that it can be used to quickly restart the dataset without rebuilding the cache. Note how much faster it is the second time:\n",
    "\n",
    "\n",
    "```\n",
    "ds = image_label_ds.cache(filename='./cache.tf-data')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 0.15 s\n",
      "12607 images: 0.1553668975830078 s\n",
      "81143.41083 Images/s\n",
      "Total time: 0.15536808967590332s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "ds = ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "#path_label_ds = path_label_ds.cache(filename='./cache.tf-path')\n",
    "\n",
    "ds = ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "ds = ds.cache(filename='./cache.tf-ds')\n",
    "ds = ds.prefetch(buffer_size=1) # Only on CPU\n",
    "#image_label_ds = image_label_ds.apply(tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=1)) # Must be final Dataset in input pipeline\n",
    "timeit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images: 0.14 s\n",
      "12607 images: 0.14266276359558105 s\n",
      "88369.24003 Images/s\n",
      "Total time: 0.14266395568847656s\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "path_label_ds = path_label_ds.cache(filename='./cache.tf-path')\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "image_label_ds = image_label_ds.cache(filename='./cache.tf-image')\n",
    "image_label_ds = image_label_ds.prefetch(buffer_size=10) # Only on CPU\n",
    "#image_label_ds = image_label_ds.apply(tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=1)) # Must be final Dataset in input pipeline\n",
    "timeit(image_label_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
