{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction for classic CNNs\n",
    "1. [CNN](#CNN)\n",
    "2. [LeNet](#LeNet)\n",
    "2. [AlexNet](#AlexNet)\n",
    "3. [Network in Network](#Network-in-Network)\n",
    "4. [VGGNet](#VGGNet)\n",
    "5. [SPPNet](#SPPNet)\n",
    "6. [Hands-on](#Hands-on)\n",
    "7. [Reference](#Reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    " - [深度學習：CNN原理](https://medium.com/@CinnamonAITaiwan/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-cnn%E5%8E%9F%E7%90%86-keras%E5%AF%A6%E7%8F%BE-432fd9ea4935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "[_\"Gradient-based learning applied to document recognition\"_](https://ieeexplore.ieee.org/document/726791)\n",
    "LeCun, Yann, et al. Proceedings of the IEEE 86.11 (1998): 2278-2324.\n",
    "- Pattern Recognition\n",
    "- Case study: handwritten character recognition \n",
    "\n",
    "\n",
    "![lenet_1](https://drive.google.com/uc?export=view&id=1hGXdbMVvqZdBXj88IbEIGpUmKcrPsdg1)\n",
    "![lenet_2](https://drive.google.com/uc?export=view&id=166V2AbS4rNSkDVhPNud1p1aOCUyqLP80)\n",
    "\n",
    "### Model Architecture\n",
    "#### Overview\n",
    "![lenet_3](https://drive.google.com/uc?export=view&id=1VgwvElx42Z8hvdpnMw-Dpo6LzkCBHOKX)\n",
    "#### Feature map between S2 and C3\n",
    "- Each Column Indicates Which Feature Map in **S2** Are Combined by the Units in a Particular Feature Map of **C3**\n",
    "![lenet_4](https://drive.google.com/uc?export=view&id=1sV4A_Wcke56jOqWV7SpEJJ5oUai212U_)\n",
    "### Demo\n",
    "![lenet_5](https://drive.google.com/uc?export=view&id=13cDxc-Ewekqnk6G1JmVN8a-WTx2B1sfR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "#Build LetNet model with Keras\n",
    "def LetNet(width, height, depth, classes):\n",
    "    # initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # first layer, convolution and pooling\n",
    "    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=6, strides=(1,1), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # second layer, convolution and pooling\n",
    "    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=16, strides=(1,1), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Fully connection layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120,activation = 'tanh'))\n",
    "    model.add(Dense(84,activation = 'tanh'))\n",
    "\n",
    "    # softmax classifier\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "LetNet_model = LetNet(224, 224, 3, 100)\n",
    "LetNet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AlexNet\n",
    "[_\"Imagenet classification with deep convolutional neural networks.\"_](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. Advances in neural information processing systems. 2012.\n",
    "\n",
    "- To learn about thousands of objects from millions of images, we need a model with a large learning capacity \n",
    "- Highly-optimized GPU implementation of 2D convolution\n",
    "\n",
    "### Model Architecture\n",
    "#### Overview\n",
    "- 8-layers: 5 Conv + 3 FC\n",
    "- The output of the last fc layer is fed to a 1000-way softmax (1000 classes)\n",
    "- The kernels of the 3rd convolutional layer are connected to all kernel maps in the 2nd layer.\n",
    "![alexnet_1](https://drive.google.com/uc?export=view&id=1zfo4ehBmgfs7zxoa6YmpATAwV3L7q1kS)\n",
    "\n",
    "#### tanh vs ReLU\n",
    "- A four-layer convolutional neural network with ReLUs (solid line) \n",
    "reaches a 25% training error rate on CIFAR-10 six times faster than \n",
    "an equivalent network with tanh neurons (dashed line).\n",
    "\n",
    "![alexnet_2](https://drive.google.com/uc?export=view&id=1-Lj2xRBcdLM7RgjvHKHw_4HbpzdwvmpB)\n",
    "\n",
    "### Training Detail\n",
    "- GTX 580 *2,  3GB of memory, 1.2 million training examples\n",
    "- Local Response Normalization\n",
    "    - reduces our top-1 and top-5 error rates by 1.4% and 1.2% \n",
    "- Overlapping Pooling \n",
    "    - size = 3, stride = 2 \n",
    "    - reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively \n",
    "- Stochastic gradient descent\n",
    "    - batch size of 128, momentum of 0.9, and weight decay of 0.0005.\n",
    "- Data Augmentation\n",
    "    - Generating image translations (crop) and horizontal reflections \n",
    "    - Extracting random 224 x 224 patches from the 256 x 256 images \n",
    "    - Altering the intensities of the RGB channels in training images \n",
    "- Dropout (0.5) in the first 2 fc layers\n",
    "\n",
    "#### Local Response Normalization\n",
    "![alexnet_3](https://drive.google.com/uc?export=view&id=1MgnMHontPOOkp7Kh7vf1rUu1g0yHrSWK)\n",
    "![alexnet_4](https://drive.google.com/uc?export=view&id=1do116Pe5bTmY2m2J9rpeVXqxgc2MLnGR)\n",
    "\n",
    "### Results\n",
    "- ILSVRC-2010\n",
    "![alexnet_5](https://drive.google.com/uc?export=view&id=1RgdRVPzvuyF9VL1cYZuqGPOTBvYPBIXF)\n",
    "- ILSVRC-2012\n",
    "![alexnet_6](https://drive.google.com/uc?export=view&id=1t7hxnNCT6BNc1iwoxG-ZfAAXQCHPaxWO)\n",
    "![alexnet_7](https://drive.google.com/uc?export=view&id=1zTu4uj3uE86DhBE0RKKq54396RrVVN9S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "#Build AlexNet model\n",
    "def AlexNet(width, height, depth, classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #First Convolution and Pooling layer\n",
    "    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(width,height,depth),padding='valid',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    \n",
    "    #Second Convolution and Pooling layer\n",
    "    model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    \n",
    "    #Three Convolution layer and Pooling Layer\n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    \n",
    "    #Fully connection layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #Classfication layer\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "\n",
    "    return model\n",
    "  \n",
    "AlexNet_model = AlexNet(224, 224, 3, 100)\n",
    "AlexNet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Network in Network\n",
    "[_\"Network in network.\"_](https://arxiv.org/abs/1312.4400.pdf) Lin, Min, Qiang Chen, and Shuicheng Yan.  arXiv preprint arXiv:1312.4400 (2013). \n",
    "\n",
    "- Enhance model discriminability for local patches within the receptive field \n",
    "\n",
    "### Model Architecture\n",
    "#### Overview\n",
    "![nin_2](https://drive.google.com/uc?export=view&id=1wNbDEBXHQYIvU8RzKuHTrO1NtYT0ggpC)\n",
    "\n",
    "#### mlpconv\n",
    "![nin_1](https://drive.google.com/uc?export=view&id=19rednh6LDUSCaC2ELTYOHchHlIYhn2M_)\n",
    "\n",
    "#### Global Average Pooling\n",
    "\n",
    "### Training\n",
    "#### The regularization effect of dropout in between mlpconv layers\n",
    "![nin_5](https://drive.google.com/uc?export=view&id=1HZxYpBsbFEN22iol338Ay7wPj1TV_2BP)\n",
    "\n",
    "### Result - CIFAR-10\n",
    "![nin_3](https://drive.google.com/uc?export=view&id=1B0fW_Om5n_y2OZxu_gTpH0kbq8EF96hQ)\n",
    "![nin_4](https://drive.google.com/uc?export=view&id=12Y2w95CO0bKU3bDmFdiCiX9aAy9sBpMz)\n",
    "\n",
    "### Result - CIFAR-100\n",
    "![nin_6](https://drive.google.com/uc?export=view&id=1iIjMqW_c5qShrbcML3SKPML3QT1LZmZj)\n",
    "\n",
    "### Result - SVHN \n",
    "![nin_7](https://drive.google.com/uc?export=view&id=1678sieIHCe4YpeD2BOx3SpTTxp43Jyud)\n",
    "![nin_8](https://drive.google.com/uc?export=view&id=1KuW276jAlfsQLmp17HkMX6D890I6OSi9)\n",
    "\n",
    "### Result - MNIST \n",
    "![nin_9](https://drive.google.com/uc?export=view&id=1JXRnNIB7rFTsz_85fDVMPzLsYXp722W6)\n",
    "\n",
    "### Global Average Pooling vs fc layer\n",
    "![nin_10](https://drive.google.com/uc?export=view&id=1MHhYiV5eQcP4louHuY4LiVPE2dyOC7Sn)\n",
    "\n",
    "### Visualization\n",
    "![nin_11](https://drive.google.com/uc?export=view&id=1fYBBRRvuH-pY_UyMJlXcUSskkG6iN006)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, BatchNormalization, MaxPool2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "weight_decay = 1e-6\n",
    "\n",
    "def mlpconv(model, conv_config):\n",
    "    model.add(Conv2D(192,(5,5),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal',\n",
    "                     input_shape=conv_config['conv2d_0_dim']))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(160,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(96,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPool2D(pool_size=(3,3),strides=(2,2),padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    return model\n",
    "    \n",
    "def NIN(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    conv_config_0 = {\n",
    "        'conv2d_0_dim': (width, height, depth),  # GLM\n",
    "        'conv2d_1_dim': (1, 1, 192),  # 1 X 1\n",
    "        'conv2d_2_dim': (1, 1, 96),  # 1 X 1\n",
    "    }\n",
    "    conv_config_1 = {\n",
    "        'conv2d_0_dim': (5, 5, 192),  # GLM\n",
    "        'conv2d_1_dim': (1, 1, 192),  # 1 X 1\n",
    "        'conv2d_2_dim': (1, 1, 96),  # 1 X 1\n",
    "    }\n",
    "    conv_config_2 = {\n",
    "        'conv2d_0_dim': (5, 5, 192),  # GLM\n",
    "        'conv2d_1_dim': (1, 1, 192),  # 1 X 1\n",
    "        'conv2d_2_dim': (1, 1, classes),  # 1 X 1\n",
    "    }\n",
    "    model = mlpconv(model, conv_config_0)\n",
    "    model = mlpconv(model, conv_config_1)\n",
    "    model = mlpconv(model, conv_config_2)\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "    \n",
    "#     model.add(Conv2D(192,(5,5),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal',\n",
    "#                      input_shape=(width,height,depth)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(160,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(96,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPool2D(pool_size=(3,3),strides=(2,2),padding='same'))\n",
    "\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv2D(192,(5,5),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(192,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(192,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPool2D(pool_size=(3,3),strides=(2,2),padding='same'))\n",
    "    \n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv2D(192,(3,3),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(192,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(classes,(1,1),padding='same',kernel_regularizer=l2(weight_decay),kernel_initializer='he_normal'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))  \n",
    "    \n",
    "#     model.add(GlobalAveragePooling2D())\n",
    "#     model.add(Activation('softmax'))\n",
    "#     return model\n",
    "\n",
    "NIN_model = NIN(224, 224, 3, 100)\n",
    "NIN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## VGGNet\n",
    "[_\"Very deep convolutional networks for large-scale image recognition.\"_](https://arxiv.org/abs/1409.1556/) Simonyan, Karen, and Andrew Zisserman. arXiv preprint arXiv:1409.1556(2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SPPNet\n",
    "[_\"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\"_](https://arxiv.org/abs/1406.4729) He, K., Zhang, X., Ren, S. and Sun, J. arXiv:1406.472(2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reference\n",
    "> [[機器學習 ML NOTE] CNN演化史(AlexNet、VGG、Inception、ResNet)+Keras Coding]( https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306)\n",
    "\n",
    "> [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cin_env_py36",
   "language": "python",
   "name": "cin_env_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
