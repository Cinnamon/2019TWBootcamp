{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction for classic CNNs\n",
    "1. [CNN](#CNN)\n",
    "2. [LeNet](#LeNet)\n",
    "2. [AlexNet](#AlexNet)\n",
    "3. [Network in Network](#Network-in-Network)\n",
    "4. [VGGNet](#VGGNet)\n",
    "5. [SPPNet](#SPPNet)\n",
    "6. [Hands-on](#Hands-on)\n",
    "7. [Reference](#Reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    " - [深度學習：CNN原理](https://medium.com/@CinnamonAITaiwan/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-cnn%E5%8E%9F%E7%90%86-keras%E5%AF%A6%E7%8F%BE-432fd9ea4935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "[_\"Gradient-based learning applied to document recognition\"_](https://ieeexplore.ieee.org/document/726791)\n",
    "LeCun, Yann, et al. Proceedings of the IEEE 86.11 (1998): 2278-2324.\n",
    "- Pattern Recognition\n",
    "- Case study: handwritten character recognition \n",
    "\n",
    "\n",
    "![lenet_1](https://drive.google.com/uc?export=view&id=1hGXdbMVvqZdBXj88IbEIGpUmKcrPsdg1)\n",
    "![lenet_2](https://drive.google.com/uc?export=view&id=166V2AbS4rNSkDVhPNud1p1aOCUyqLP80)\n",
    "\n",
    "### Model Architecture\n",
    "#### Overview\n",
    "![lenet_3](https://drive.google.com/uc?export=view&id=1VgwvElx42Z8hvdpnMw-Dpo6LzkCBHOKX)\n",
    "#### Feature map between S2 and C3\n",
    "- Each Column Indicates Which Feature Map in **S2** Are Combined by the Units in a Particular Feature Map of **C3**\n",
    "![lenet_4](https://drive.google.com/uc?export=view&id=1sV4A_Wcke56jOqWV7SpEJJ5oUai212U_)\n",
    "\n",
    "### Demo\n",
    "![lenet_5](https://drive.google.com/uc?export=view&id=13cDxc-Ewekqnk6G1JmVN8a-WTx2B1sfR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/blake_cinnamon/Venv/cin_env_py36_tf1_13/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 220, 220, 6)       456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 106, 106, 16)      2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 53, 53, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 44944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               5393400   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               8500      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 5,414,936\n",
      "Trainable params: 5,414,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "#Build LetNet model with Keras\n",
    "def LetNet(width, height, depth, classes):\n",
    "    # initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # first layer, convolution and pooling\n",
    "    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=6, strides=(1,1), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # second layer, convolution and pooling\n",
    "    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=16, strides=(1,1), activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Fully connection layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120,activation = 'tanh'))\n",
    "    model.add(Dense(84,activation = 'tanh'))\n",
    "\n",
    "    # softmax classifier\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "LetNet_model = LetNet(224, 224, 3, 100)\n",
    "LetNet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AlexNet\n",
    "[_\"Imagenet classification with deep convolutional neural networks.\"_](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. Advances in neural information processing systems. 2012.\n",
    "\n",
    "- To learn about thousands of objects from millions of images, we need a model with a large learning capacity \n",
    "- Highly-optimized GPU implementation of 2D convolution\n",
    "\n",
    "### Model Architecture\n",
    "#### Overview\n",
    "- 8-layers: 5 Conv + 3 FC\n",
    "- The output of the last fc layer is fed to a 1000-way softmax (1000 classes)\n",
    "- The kernels of the 3rd convolutional layer are connected to all kernel maps in the 2nd layer.\n",
    "![alexnet_1](https://drive.google.com/uc?export=view&id=1zfo4ehBmgfs7zxoa6YmpATAwV3L7q1kS)\n",
    "\n",
    "#### tanh vs ReLU\n",
    "- A four-layer convolutional neural network with _ReLUs_ (solid line) <br>\n",
    "reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with _tanh_ neurons (dashed line).\n",
    "\n",
    "![alexnet_2](https://drive.google.com/uc?export=view&id=1-Lj2xRBcdLM7RgjvHKHw_4HbpzdwvmpB)\n",
    "\n",
    "### Training Detail\n",
    "- GTX 580 *2,  3GB of memory, 1.2 million training examples\n",
    "- Local Response Normalization\n",
    "    - reduces our top-1 and top-5 error rates by 1.4% and 1.2% \n",
    "- Overlapping Pooling \n",
    "    - size = 3, stride = 2 \n",
    "    - reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively \n",
    "- Stochastic gradient descent\n",
    "    - batch size of 128, momentum of 0.9, and weight decay of 0.0005.\n",
    "- Data Augmentation\n",
    "    - Generating image translations (crop) and horizontal reflections \n",
    "    - Extracting random 224 x 224 patches from the 256 x 256 images \n",
    "    - Altering the intensities of the RGB channels in training images \n",
    "- Dropout (0.5) in the first 2 fc layers\n",
    "\n",
    "#### Local Response Normalization\n",
    "![alexnet_3](https://drive.google.com/uc?export=view&id=1MgnMHontPOOkp7Kh7vf1rUu1g0yHrSWK)\n",
    "![alexnet_4](https://drive.google.com/uc?export=view&id=1do116Pe5bTmY2m2J9rpeVXqxgc2MLnGR)\n",
    "\n",
    "### Results\n",
    "- ILSVRC-2010\n",
    "![alexnet_5](https://drive.google.com/uc?export=view&id=1RgdRVPzvuyF9VL1cYZuqGPOTBvYPBIXF)\n",
    "- ILSVRC-2012\n",
    "![alexnet_6](https://drive.google.com/uc?export=view&id=1t7hxnNCT6BNc1iwoxG-ZfAAXQCHPaxWO)\n",
    "![alexnet_7](https://drive.google.com/uc?export=view&id=1zTu4uj3uE86DhBE0RKKq54396RrVVN9S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/blake_cinnamon/Venv/cin_env_py36_tf1_13/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              26218496  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               100100    \n",
      "=================================================================\n",
      "Total params: 50,944,108\n",
      "Trainable params: 50,944,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "#Build AlexNet model\n",
    "def AlexNet(width, height, depth, classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #First Convolution and Pooling layer\n",
    "    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(width,height,depth),padding='valid',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    \n",
    "    #Second Convolution and Pooling layer\n",
    "    model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    \n",
    "    #Three Convolution layer and Pooling Layer\n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    \n",
    "    #Fully connection layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #Classfication layer\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "\n",
    "    return model\n",
    "  \n",
    "AlexNet_model = AlexNet(224, 224, 3, 100)\n",
    "AlexNet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Network in Network\n",
    "[_\"Network in network.\"_](https://arxiv.org/abs/1312.4400.pdf) Lin, Min, Qiang Chen, and Shuicheng Yan.  arXiv preprint arXiv:1312.4400 (2013). \n",
    "\n",
    "- Enhance model discriminability for local patches within the receptive field \n",
    "\n",
    "### Model Architecture\n",
    "#### Overview\n",
    "![nin_2](https://drive.google.com/uc?export=view&id=1wNbDEBXHQYIvU8RzKuHTrO1NtYT0ggpC)\n",
    "\n",
    "#### mlpconv\n",
    "![nin_1](https://drive.google.com/uc?export=view&id=19rednh6LDUSCaC2ELTYOHchHlIYhn2M_)\n",
    "\n",
    "#### Global Average Pooling\n",
    "- Generate one feature map for each corresponding category\n",
    "- Take the average of each feature map, and the resulting vector is fed directly into the softmax layer\n",
    "\n",
    "\n",
    "### Training\n",
    "#### The regularization effect of dropout in between mlpconv layers\n",
    "![nin_5](https://drive.google.com/uc?export=view&id=1HZxYpBsbFEN22iol338Ay7wPj1TV_2BP)\n",
    "\n",
    "### Result - CIFAR-10\n",
    "![nin_3](https://drive.google.com/uc?export=view&id=1B0fW_Om5n_y2OZxu_gTpH0kbq8EF96hQ)\n",
    "![nin_4](https://drive.google.com/uc?export=view&id=12Y2w95CO0bKU3bDmFdiCiX9aAy9sBpMz)\n",
    "\n",
    "### Result - CIFAR-100\n",
    "![nin_6](https://drive.google.com/uc?export=view&id=1iIjMqW_c5qShrbcML3SKPML3QT1LZmZj)\n",
    "\n",
    "### Result - SVHN \n",
    "![nin_7](https://drive.google.com/uc?export=view&id=1678sieIHCe4YpeD2BOx3SpTTxp43Jyud)\n",
    "![nin_8](https://drive.google.com/uc?export=view&id=1KuW276jAlfsQLmp17HkMX6D890I6OSi9)\n",
    "\n",
    "### Result - MNIST \n",
    "![nin_9](https://drive.google.com/uc?export=view&id=1JXRnNIB7rFTsz_85fDVMPzLsYXp722W6)\n",
    "\n",
    "### Global Average Pooling vs fc layer\n",
    "![nin_10](https://drive.google.com/uc?export=view&id=1MHhYiV5eQcP4louHuY4LiVPE2dyOC7Sn)\n",
    "\n",
    "### Visualization\n",
    "![nin_11](https://drive.google.com/uc?export=view&id=1fYBBRRvuH-pY_UyMJlXcUSskkG6iN006)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 224, 224, 192)     14592     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 192)     768       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 224, 224, 192)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 224, 224, 192)     37056     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 224, 224, 192)     768       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 224, 224, 192)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 224, 224, 96)      18528     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 224, 224, 96)      384       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 224, 224, 96)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 96)      0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 112, 112, 96)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 112, 112, 192)     460992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 112, 112, 192)     768       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 112, 112, 192)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 112, 112, 192)     37056     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 112, 112, 192)     768       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 112, 112, 192)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 112, 112, 96)      18528     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 112, 112, 96)      384       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 112, 112, 96)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 56, 56, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 56, 56, 192)       460992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 56, 56, 192)       768       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 56, 56, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 56, 56, 192)       37056     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 56, 56, 192)       768       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 56, 56, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 56, 56, 100)       19300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 56, 56, 100)       400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 56, 56, 100)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 100)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 28, 28, 100)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 1,109,876\n",
      "Trainable params: 1,106,988\n",
      "Non-trainable params: 2,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, BatchNormalization, MaxPool2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "weight_decay = 1e-6\n",
    "\n",
    "def mlpconv(model, conv_config):\n",
    "    \n",
    "    w, h, c = conv_config['conv2d_0_dim']\n",
    "    if 'input_shape' in conv_config:\n",
    "        model.add(Conv2D(c, (w, h), padding='same',\n",
    "                         kernel_regularizer=l2(weight_decay),\n",
    "                         kernel_initializer='he_normal',\n",
    "                         input_shape=conv_config['input_shape']))\n",
    "    else:\n",
    "        model.add(Conv2D(c, (w, h), padding='same',\n",
    "                         kernel_regularizer=l2(weight_decay),\n",
    "                         kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    w, h, c = conv_config['conv2d_1_dim']\n",
    "    model.add(Conv2D(c, (w, h), padding='same',\n",
    "                     kernel_regularizer=l2(weight_decay),\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    w, h, c = conv_config['conv2d_2_dim']    \n",
    "    model.add(Conv2D(c, (w, h), padding='same',\n",
    "                     kernel_regularizer=l2(weight_decay),\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPool2D(pool_size=(3,3),strides=(2,2),padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    return model\n",
    "    \n",
    "def NIN(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    conv_config_0 = {\n",
    "        'input_shape': (width, height, depth),  # GLM\n",
    "        'conv2d_0_dim': (5, 5, 192),  # GLM\n",
    "        'conv2d_1_dim': (1, 1, 192),  # 1 X 1\n",
    "        'conv2d_2_dim': (1, 1, 96),  # 1 X 1\n",
    "    }\n",
    "    conv_config_1 = {\n",
    "        'conv2d_0_dim': (5, 5, 192),  # GLM\n",
    "        'conv2d_1_dim': (1, 1, 192),  # 1 X 1\n",
    "        'conv2d_2_dim': (1, 1, 96),  # 1 X 1\n",
    "    }\n",
    "    conv_config_2 = {\n",
    "        'conv2d_0_dim': (5, 5, 192),  # GLM\n",
    "        'conv2d_1_dim': (1, 1, 192),  # 1 X 1\n",
    "        'conv2d_2_dim': (1, 1, classes),  # 1 X 1\n",
    "    }\n",
    "    model = mlpconv(model, conv_config_0)\n",
    "    model = mlpconv(model, conv_config_1)\n",
    "    model = mlpconv(model, conv_config_2)\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "NIN_model = NIN(224, 224, 3, 100)\n",
    "NIN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## VGGNet\n",
    "[_\"Very deep convolutional networks for large-scale image recognition.\"_](https://arxiv.org/abs/1409.1556/) Simonyan, Karen, and Andrew Zisserman. arXiv preprint arXiv:1409.1556(2015).\n",
    "\n",
    "- Increasing **depth** using an architecture with very **small** (3×3) convolution filters\n",
    "\n",
    "### 3x3 Filters\n",
    "- 1 layer of 5×5 filter, number of parameters = 5×5=25\n",
    "- 2 layers of 3×3 filters, number of parameters = 3×3+3×3=18\n",
    "\n",
    "    **Number of parameters is reduced by 28%**\n",
    "\n",
    "![vgg_1](https://drive.google.com/uc?export=view&id=1N4RqAWIgd4wwnH28w5PHEFwIjn_ttomk)\n",
    "\n",
    "### Model Architecture\n",
    "![vgg_2](https://drive.google.com/uc?export=view&id=1h6cMk3K35BHHaXiByz2ElxX8rQLlTp2r)\n",
    "\n",
    "### Training\n",
    "- Batch size = 256\n",
    "- Gradient descent with momentum = 0.9\n",
    "- Weight decay ( L2 penalty multiplier set to 5e−4)\n",
    "- Dropout = 0.5, for first 2 fc\n",
    "- Learning rate = 0.01, decreased by a factor of 10 when val accuracy stops improving\n",
    "\n",
    "#### Data\n",
    "- Crop size = 224 x 224\n",
    "- Single-scale training:\n",
    "    - fixed ‘S’ = 256 or 384, ‘S’: the smallest side of an image\n",
    "- Multi-scale training:\n",
    "    - ‘S’ in range [Smin, Smax], Smin = 256, Smax = 512\n",
    "- Pre-train multi-scale models by a single-scale model with the same configuration, with ‘S’ = 384 .\n",
    "\n",
    "### Results\n",
    "![vgg_3](https://drive.google.com/uc?export=view&id=15md49vDtOx2-6wOvdMW9Uldti6GRJui4)\n",
    "![vgg_4](https://drive.google.com/uc?export=view&id=1UpKl1AqUWJ-OhsozI__Y-Z1vV_Irmy9b)\n",
    "- LRN does not improve on Model-A without any normalization layers\n",
    "- Classification error decreases with increased Conv depth\n",
    "\n",
    "![vgg_4](https://drive.google.com/uc?export=view&id=1sESpvs676Db6UBbSErtOUAax9buOIIYs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 112, 112, 128)     49280     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 17)                17017     \n",
      "=================================================================\n",
      "Total params: 138,349,985\n",
      "Trainable params: 138,349,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def VGG16Net(width, height, depth, classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(17,activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "  \n",
    "VGG16_model = VGG16Net(224, 224, 3, 100)\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SPPNet\n",
    "[_\"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\"_](https://arxiv.org/abs/1406.4729) He, K., Zhang, X., Ren, S. and Sun, J. arXiv:1406.472(2014).\n",
    "\n",
    "- Cons: Fixed input image size (e.g., 224×224), which limits both the aspect ratio and the scale of the input image\n",
    "- SPP-net generates a fixed-length representation regardless of image size/scale\n",
    "\n",
    "![spp_1](https://drive.google.com/uc?export=view&id=1MP-4L3v8EeV8UnXiTE2gUPhCbi-4uaO2)\n",
    "<!--\n",
    "![spp_2](https://drive.google.com/uc?export=view&id=1z77Pu-oYdsRSMAA-jAz8T6Q8vLNGSAs7)\n",
    "--->\n",
    "\n",
    "### Model Architecture\n",
    "![spp_3](https://drive.google.com/uc?export=view&id=1Mg7cmrPz3Lubn9aMtQAb5Z78G6dgV1Xa)\n",
    "\n",
    "### Training\n",
    "- Single-size training\n",
    "    - fixed-size input (224×224) cropped from images\n",
    "    - compute the bin sizes first\n",
    "\n",
    "- Multi-size training\n",
    "    - consider two sizes: 180×180 in addition to 224×224\n",
    "    - train each full epoch on one network, and then switch to the other one (keeping all weights)\n",
    "    - resize instead of crop\n",
    "    \n",
    "- Data Augmentation\n",
    "    - Horizontal flipping\n",
    "    - Color altering\n",
    "    - Dropout on 2 fc\n",
    "    \n",
    "- **NOTE** <br>\n",
    "The above single/multi-size solutions are for training only. <br>\n",
    "At the testing stage, it is straightforward to apply SPP-net on images of any sizes.\n",
    "\n",
    "### Results\n",
    "- ImageNet 2012 (standard 10-view)\n",
    "![spp_4](https://drive.google.com/uc?export=view&id=1N1y-9NhJ3fjCa5W0jbIU4ChxwXhXMDPc)\n",
    "\n",
    "- ImageNet 2012 (single view)\n",
    "![spp_5](https://drive.google.com/uc?export=view&id=1YtlIB1zun2yI94BgoORxz-RbbY1Ic7M3)\n",
    "\n",
    "- ImageNet 2012 (multi-view)\n",
    "![spp_6](https://drive.google.com/uc?export=view&id=1uDO6EJGKZmLgy6Kr2ftrs5AWOZ7WZQRi)\n",
    "\n",
    "- ImageNet 2014 :(\n",
    "![spp_6](https://drive.google.com/uc?export=view&id=1Jx7otVjOm0LyTdbEgpEwkJsIZUsdYaC7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(1, 21), dtype=float32)\n",
      "Tensor(\"concat_1:0\", shape=(1, 21), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "def SPP(x, pool_list=[1, 2, 4]):\n",
    "    x = K.variable(x)\n",
    "    outputs = []\n",
    "    input_shape = K.shape(x)\n",
    "    num_batch, num_rows, num_cols, depth = input_shape[0], input_shape[1], input_shape[2], input_shape[3]\n",
    "    row_length = [K.cast(num_rows, 'float32') / i for i in pool_list]\n",
    "    col_length = [K.cast(num_cols, 'float32') / i for i in pool_list]\n",
    "        \n",
    "    for pool_num, num_pool_regions in enumerate(pool_list):\n",
    "        for jy in range(num_pool_regions):\n",
    "            for ix in range(num_pool_regions):\n",
    "                x1 = ix * col_length[pool_num]\n",
    "                x2 = ix * col_length[pool_num] + col_length[pool_num]\n",
    "                y1 = jy * row_length[pool_num]\n",
    "                y2 = jy * row_length[pool_num] + row_length[pool_num]\n",
    "\n",
    "                x1 = K.cast(K.round(x1), 'int32')\n",
    "                x2 = K.cast(K.round(x2), 'int32')\n",
    "                y1 = K.cast(K.round(y1), 'int32')\n",
    "                y2 = K.cast(K.round(y2), 'int32')\n",
    "\n",
    "                new_shape = [num_batch, y2 - y1, x2 - x1, depth]\n",
    "                x_crop = x[:, y1:y2, x1:x2, :]\n",
    "\n",
    "                xm = K.reshape(x_crop, new_shape)\n",
    "                pooled_val = K.max(xm, axis=(1, 2))\n",
    "                outputs.append(pooled_val)\n",
    "\n",
    "    outputs = K.concatenate(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "print(SPP(np.random.rand(1, 224, 224, 1), pool_list=[1, 2, 4]))\n",
    "print(SPP(np.random.rand(1, 164, 164, 1), pool_list=[1, 2, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reference\n",
    "> [[機器學習 ML NOTE] CNN演化史(AlexNet、VGG、Inception、ResNet)+Keras Coding]( https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306)\n",
    "\n",
    "> [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)\n",
    "\n",
    "> https://github.com/yhenon/keras-spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
