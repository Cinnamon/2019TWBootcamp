{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Object Detection\n",
    "1. [Overview](#Overview)\n",
    "\n",
    "3. [R-CNN Family](#R-CNN-Family)\n",
    "    - [R-CNN](#R-CNN)\n",
    "    - [Fast R-CNN](#Fast-R-CNN)\n",
    "    - [Faster R-CNN](#Faster-R-CNN)\n",
    "    \n",
    "4. [YOLO Family](#YOLO-Family)\n",
    "    - [YOLO v1](#YOLO-v1)\n",
    "    - [YOLO v2](#YOLO-v2)\n",
    "    - [YOLO v3](#YOLO-v3)\n",
    "    \n",
    "5. [Appendix](#Appendix)\n",
    "7. [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Computer vision tasks\n",
    "![cv_tasks](https://drive.google.com/uc?export=view&id=1ZuXHkVFyfxzgtBOncmhAlNz1uH80q5z-)\n",
    "- **Image Classification**\n",
    "    - What is the object in the image?\n",
    "- **Object Localization**\n",
    "    - Where is the object in the image?\n",
    "- **Object Detection**\n",
    "    - Classification + Localization\n",
    "- **Segmentation**\n",
    "    - Pixel-wise predictions\n",
    "    \n",
    "    \n",
    "### Two Stage vs One Stage\n",
    "- Two Stage: __Region Proposal__ first, then __Classification__\n",
    "![Two Stage](https://drive.google.com/uc?export=view&id=1qh8ynXBFm_rZozWsG6_57Ta2rLV22dV2)\n",
    "\n",
    "- One Stage: End-to-End\n",
    "![One Stage](https://drive.google.com/uc?export=view&id=1A33ZiA3EscSMemPeCPFg-WfAXmbj-IYL)\n",
    "\n",
    "|Method|Pros|Cons|\n",
    "|----|----|----|\n",
    "|Two Stage|High Accuracy| Slow |\n",
    "|One Stage|Low Accuracy| Fast, Real-time |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## R-CNN Family\n",
    "\n",
    "### R-CNN\n",
    "[_\"Rich feature hierarchies for accurate object detection and semantic segmentation.\"_](https://arxiv.org/abs/1311.2524) Girshick, R., Donahue, J., Darrell, T. and Malik, J. (2014)\n",
    "\n",
    "#### Model\n",
    "![r-cnn-1](https://drive.google.com/uc?export=view&id=1wDB75843Do-uspPNkl78Yenltd64s9jB)\n",
    "\n",
    "- Region proposals: [Selective search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "- Feature extraction: CNN, AlexNet\n",
    "\n",
    "- Non-maximum suppresion\n",
    "![NMS](https://drive.google.com/uc?export=view&id=1Gf_cBFRlrd8pFmhLt5Bu0cESeH8MckIz)\n",
    "![IOU](https://drive.google.com/uc?export=view&id=14PYqELFvKWkQFzNkF9LXm0KA2d8Z1qHU)\n",
    "\n",
    "\n",
    "### Fast R-CNN\n",
    "[_\"Fast R-CNN\"_](https://arxiv.org/abs/1504.08083)Girshick, R. (2015)\n",
    "\n",
    "#### Cons of R-CNN\n",
    "- Training is a multi-stage pipeline\n",
    "- Training is expensive in space and time\n",
    "- Object detection is slow\n",
    "\n",
    "#### Model\n",
    "![r-cnn-1](https://drive.google.com/uc?export=view&id=19vEeoYVROlCi4h3S59s95hnybuy1as9v)\n",
    "\n",
    "#### RoI pooling layer\n",
    "![spp_3](https://drive.google.com/uc?export=view&id=1Mg7cmrPz3Lubn9aMtQAb5Z78G6dgV1Xa)\n",
    "\n",
    "![roi pooling](https://drive.google.com/uc?export=view&id=1d0myxH-6YjKrv1W5uKpjHvQLBUEAUtT9)\n",
    "\n",
    "#### ROI Align (Mask R-CNN)\n",
    "![roi align 1](https://drive.google.com/uc?export=view&id=1ybpWdrXlSA40bKD7IcbJdFbipF2QttmW)\n",
    "\n",
    "![bi-linear](https://drive.google.com/uc?export=view&id=1Yfz8m86rVaS6_gBnAmb3M5YGXWZ9B5bB)\n",
    "![bi-linear-f1](https://drive.google.com/uc?export=view&id=1HgXGfKC8dOeNqGTAznaGMwpaa0-vYcIT)\n",
    "![bi-linear-f2](https://drive.google.com/uc?export=view&id=1NJzGrm7Mb7uOBZqrPspcjIes77vCHq5B)\n",
    "![bi-linear-f3](https://drive.google.com/uc?export=view&id=17aqLveNsWF8KAsjP0-YR9f4JBe5oLFuA)\n",
    "\n",
    "\n",
    "### Faster R-CNN\n",
    "[_\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.\"_](https://arxiv.org/abs/1506.01497) Ren, S., He, K., Girshick, R. and Sun, J. (2015)\n",
    "\n",
    "#### Model\n",
    "![faster r-cnn-1](https://drive.google.com/uc?export=view&id=1EZeno8hyt3fMhN_z4WNqFICM7UO1XjFY)\n",
    "\n",
    "#### Region Proposal Network\n",
    "![rpn](https://drive.google.com/uc?export=view&id=1EZeno8hyt3fMhN_z4WNqFICM7UO1XjFY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## YOLO Family\n",
    "\n",
    "### YOLO v1\n",
    "[_\"You Only Look Once: Unified, Real-Time Object Detection.\"_](https://arxiv.org/abs/1506.02640)Redmon, J., Divvala, S., Girshick, R. and Farhadi, A. (2015)\n",
    "\n",
    "#### Model\n",
    "![yolo_v1_0](https://drive.google.com/uc?export=view&id=1IZaW3SpLFtuP3JuWbQq56LyRInmYnSNi)\n",
    "\n",
    "![yolo_v1_2](https://drive.google.com/uc?export=view&id=1BUAUYF3x9cJheqQfSdwT-9w07B0VNX_5)\n",
    "\n",
    "- tensor dimenstion: S × S × (B ∗ 5 + C)\n",
    "- S: W and H of tensor, 7\n",
    "- B: num bounding box, 2\n",
    "- C: num class, 20\n",
    "- 5: (x, y, w, h, confidence)\n",
    "\n",
    "\n",
    "![yolo v1 1](https://miro.medium.com/max/700/1*JniWRt-ceWLNlkOULjhdpg.png)\n",
    "\n",
    "\n",
    "### YOLO v2\n",
    "[_\"YOLO9000: Better, Faster, Stronger.\"_](https://arxiv.org/abs/1612.08242)Redmon, J. and Farhadi, A. (2016).\n",
    "\n",
    "#### Better\n",
    "- Batch Normalization\n",
    "- High Resolution Classifie (224 X 224 -> 448 X 448)\n",
    "- Convolutional With Anchor Boxes\n",
    "- Dimension Clusters (k-means)\n",
    "![k means](https://drive.google.com/uc?export=view&id=1DnGl95YPBgfr7efPuhDQYZl1o2Le_82S)\n",
    "![distance](https://miro.medium.com/max/539/1*4UeShDFUuddbOOMAh7KTdg.png)\n",
    "- Direct Location Prediction\n",
    "- Fine-Grained Features\n",
    "- Multi-Scale Training\n",
    "\n",
    "#### Faster\n",
    "- VGG-16 (30.69 billion floating point operations per 224*224 image) -> Darknet-19 (8.52 billion)\n",
    "[darknet19](https://miro.medium.com/max/548/1*iPHGuCWfCOTjrEW187fSZQ.png)\n",
    "\n",
    "#### Stronger\n",
    "- WordTree\n",
    "![wordtree_1](https://miro.medium.com/max/700/1*1rpDaEiL-4NuTBlk9p0oAg.png)\n",
    "![wordtree_2](https://miro.medium.com/max/700/1*Js9qWV9taiuZHzTgA65OxQ.png)\n",
    "![wordtree_3](https://miro.medium.com/max/700/1*YiX61mdylOzZYlBFXl9HjA.png)\n",
    "\n",
    "![it's over 9000](https://i.kym-cdn.com/entries/icons/original/000/000/056/itsover1000.jpg)\n",
    "\n",
    "\n",
    "### YOLO v3\n",
    "[_\"YOLOv3: An Incremental Improvement.\"_](https://arxiv.org/abs/1804.02767)Redmon, J. and Farhadi, A. (2018).\n",
    "\n",
    "> I managed to make some improvements to YOLO.\n",
    "> But, honestly, nothing like super interesting, just a bunch of small changes that make it better.\n",
    "> I also helped out with other people’s research a little.\n",
    ">\n",
    ">\n",
    "> So here’s the deal with YOLOv3: We mostly took good ideas from other people. \n",
    "\n",
    "#### Bounding box prediction \n",
    "- No change\n",
    "\n",
    "#### Class prediction\n",
    "- Softmax is not used\n",
    "- independent logistic classifiers are used and binary cross-entropy loss is used\n",
    "\n",
    "#### Prediction Across Scales\n",
    "- 3 different scales are used\n",
    "![fpn](https://miro.medium.com/max/690/1*D_EAjMnlR9v4LqHhEYZJLg.png)\n",
    "\n",
    "#### Feature Extractor: Darknet-53\n",
    "![darknet-53](https://miro.medium.com/max/490/1*tF1fK8-D5PVDb4khxvIH_g.png)\n",
    "\n",
    "![yolov3_1](https://miro.medium.com/max/700/1*RFpjH8D6TStBaYuZYehe_g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### [History of Object Detection](https://medium.com/@nikasa1889/the-modern-history-of-object-recognition-infographic-aea18517c318)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reference\n",
    "> [深度學習-什麼是one stage，什麼是two stage 物件偵測](https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E4%BB%80%E9%BA%BC%E6%98%AFone-stage-%E4%BB%80%E9%BA%BC%E6%98%AFtwo-stage-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-fc3ce505390f)\n",
    ">\n",
    "> [A Gentle Introduction to Object Recognition With Deep Learning\n",
    "](https://machinelearningmastery.com/object-recognition-with-deep-learning/)\n",
    ">\n",
    "> [Review of Deep Learning Algorithms for Object Detection](https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)\n",
    ">\n",
    "> [Joseph Chet Redmon's website](https://pjreddie.com/)\n",
    ">\n",
    "> [Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n",
    ">\n",
    "> [機器/深度學習: 物件偵測 Non-Maximum Suppression (NMS)](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-non-maximum-suppression-nms-aa70c45adffa)\n",
    ">\n",
    ">[Bilinear interpolation](https://wiki.mbalib.com/zh-tw/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cin_env_py36_tf1_13",
   "language": "python",
   "name": "cin_env_py36_tf1_13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
