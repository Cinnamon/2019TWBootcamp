{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        # print(shape)\n",
    "        # print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            # print(dim)\n",
    "            variable_parameters *= dim.value\n",
    "        # print(variable_parameters)\n",
    "        total_parameters += variable_parameters\n",
    "    return total_parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResentV2_block(input_data,input_depth,compress_depth,output_depth,strides=(1,1)):\n",
    "    X_shortcut=input_data\n",
    "    X=tf.layers.conv2d(input_data,compress_depth,(1,1))\n",
    "    X=tf.layers.batch_normalization(X)\n",
    "    X=tf.nn.leaky_relu(X)\n",
    "    X=tf.layers.conv2d(X,compress_depth,(3,3),padding='same',strides=strides)\n",
    "    X=tf.layers.batch_normalization(X)\n",
    "    X=tf.nn.leaky_relu(X)\n",
    "    X=tf.layers.conv2d(X,output_depth,(1,1))\n",
    "    if (input_depth !=output_depth):\n",
    "        X_shortcut=tf.layers.conv2d(X_shortcut,output_depth,(1,1),strides=strides,padding='same')\n",
    "    if (input_depth ==output_depth) & (strides !=(1,1)):\n",
    "        X_shortcut=tf.image.resize_images(X_shortcut, (X.shape[1], X.shape[2]), method=0)\n",
    "    out=X_shortcut+X\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(?, 32, 32, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 64, 64, 128])\n",
    "out=ResentV2_block(inputs,128,64,256,strides=(2,2))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Residual_Block(input_data, in_channel,out_channel,expansion = 4, s = 1):\n",
    "    X_shortcut = input_data##記住輸入\n",
    "    X = tf.layers.conv2d(input_data,out_channel, (1, 1), strides = (s,s))\n",
    "    X = tf.layers.batch_normalization(X)\n",
    "    X = tf.nn.relu(X)\n",
    "\n",
    "    X =  tf.layers.conv2d(X,out_channel, (3, 3),padding='same', strides = (s,s))\n",
    "    X = tf.layers.batch_normalization(X)\n",
    "    X =tf.nn.relu(X)\n",
    "\n",
    "    X =  tf.layers.conv2d(X,out_channel*expansion, (1,1), strides = (1, 1), )\n",
    "    X =  tf.layers.batch_normalization(X)\n",
    "    \n",
    "    if (in_channel != out_channel*expansion) or (s != 1):\n",
    "        X_shortcut=tf.layers.conv2d(X_shortcut,out_channel*expansion, (1,1), strides = (s, s))\n",
    "        X_shortcut = tf.layers.batch_normalization(X)\n",
    "\n",
    "    X = X+X_shortcut\n",
    "    X = tf.nn.relu(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 64, 64, 128])\n",
    "out=Residual_Block(inputs, 128,256,expansion = 4, s = 1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def Inception(input_data,input_depth=192):\n",
    "    with tf.name_scope('Branch_1'):\n",
    "        X_1=tf.layers.conv2d(input_data,64,(1,1))\n",
    "        X_1=tf.layers.batch_normalization(X_1)\n",
    "        X_1=tf.nn.leaky_relu(X_1)\n",
    "        \n",
    "    with tf.name_scope('Branch_2'): \n",
    "        X_2=tf.layers.conv2d(input_data,96,(1,1))\n",
    "        X_2=tf.layers.batch_normalization(X_2)\n",
    "        X_2=tf.nn.leaky_relu(X_2)\n",
    "        \n",
    "        X_2=tf.layers.conv2d(X_2,128,(3,3),padding='same')\n",
    "        X_2=tf.layers.batch_normalization(X_2)\n",
    "        X_2=tf.nn.leaky_relu(X_2)\n",
    "        \n",
    "    with tf.name_scope('Branch_3'): \n",
    "        X_3=tf.layers.conv2d(input_data,16,(1,1))\n",
    "        X_3=tf.layers.batch_normalization(X_3)\n",
    "        X_3=tf.nn.leaky_relu(X_3)\n",
    "        \n",
    "        X_3=tf.layers.conv2d(X_3,48,(3,3),padding='same')\n",
    "        X_3=tf.layers.batch_normalization(X_3)\n",
    "        X_3=tf.nn.leaky_relu(X_3)\n",
    "\n",
    "        X_3=tf.layers.conv2d(X_3,32,(5,5),padding='same')\n",
    "        X_3=tf.layers.batch_normalization(X_3)\n",
    "        X_3=tf.nn.leaky_relu(X_3)\n",
    "\n",
    "    with tf.name_scope('Branch_4'): \n",
    "        X_4=tf.layers.max_pooling2d(input_data,2,1,padding='same')\n",
    "        X_4=tf.layers.batch_normalization(X_4)\n",
    "        X_4=tf.nn.leaky_relu(X_4)\n",
    "        \n",
    "        X_4=tf.layers.conv2d(X_4,32,(1,1),padding='same')\n",
    "        X_4=tf.layers.batch_normalization(X_3)\n",
    "        X_4=tf.nn.leaky_relu(X_3)\n",
    "\n",
    "    out=tf.concat((X_1,X_2,X_3,X_4),axis=3)\n",
    "\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(?, 28, 28, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 28, 28, 192])\n",
    "out=Inception(inputs,input_depth=192)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception-Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionResentA_block(input_data,input_depth=3,output_depth=384):\n",
    "    X_shortcut=input_data\n",
    "    with tf.name_scope('Branch_1'):\n",
    "        X_1=tf.layers.conv2d(input_data,32,(1,1))\n",
    "        X_1=tf.layers.batch_normalization(X_1)\n",
    "        X_1=tf.nn.leaky_relu(X_1)\n",
    "        \n",
    "    with tf.name_scope('Branch_2'): \n",
    "        X_2=tf.layers.conv2d(input_data,32,(1,1))\n",
    "        X_2=tf.layers.batch_normalization(X_2)\n",
    "        X_2=tf.nn.leaky_relu(X_2)\n",
    "        \n",
    "        X_2=tf.layers.conv2d(X_2,32,(3,3),padding='same')\n",
    "        X_2=tf.layers.batch_normalization(X_2)\n",
    "        X_2=tf.nn.leaky_relu(X_2)\n",
    "        \n",
    "    with tf.name_scope('Branch_3'): \n",
    "        X_3=tf.layers.conv2d(input_data,32,(1,1))\n",
    "        X_3=tf.layers.batch_normalization(X_3)\n",
    "        X_3=tf.nn.leaky_relu(X_3)\n",
    "        \n",
    "        X_3=tf.layers.conv2d(X_3,48,(3,3),padding='same')\n",
    "        X_3=tf.layers.batch_normalization(X_3)\n",
    "        X_3=tf.nn.leaky_relu(X_3)\n",
    "        \n",
    "        \n",
    "        X_3=tf.layers.conv2d(X_3,64,(3,3),padding='same')\n",
    "        X_3=tf.layers.batch_normalization(X_3)\n",
    "        X_3=tf.nn.leaky_relu(X_3)\n",
    "    out=tf.concat((X_1,X_2,X_3),axis=3)\n",
    "\n",
    "    out=tf.layers.conv2d(out,output_depth,(1,1))\n",
    "\n",
    "    if (input_depth !=output_depth):\n",
    "        X_shortcut=tf.layers.conv2d(X_shortcut,output_depth,(1,1))\n",
    "\n",
    "    out=X_shortcut+out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(?, 64, 64, 384), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 64, 64, 3])\n",
    "out=InceptionResentA_block(inputs,input_depth=3,output_depth=384)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Dense_Stage(inputs_,depth=64,repeat=8):\n",
    "    for _ in range(repeat):\n",
    "        X_input=inputs_\n",
    "        X=tf.layers.conv2d(inputs_,depth,(1,1),strides=(1,1),activation=tf.nn.leaky_relu)\n",
    "        X=tf.layers.batch_normalization(X)\n",
    "        X=tf.layers.separable_conv2d(X,depth,(3,3),padding='SAME')\n",
    "        X=tf.nn.leaky_relu(X)\n",
    "        X=tf.layers.batch_normalization(X)\n",
    "        X=tf.concat([X_input,X],3)\n",
    "        inputs_=X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_7:0\", shape=(?, 64, 64, 515), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 64, 64, 3])\n",
    "out=Dense_Stage(inputs,depth=64,repeat=8)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 3])\n",
    "out=depthwise_conv(\n",
    "        inputs, kernel=3, stride=1, padding='SAME',\n",
    "        activation_fn=None, normalizer_fn=None,\n",
    "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        data_format='NHWC', scope='depthwise_conv')\n",
    "out=tf.layers.conv2d(out,64,(1,1))\n",
    "print(get_num_params()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tc\n",
    "\n",
    "slim = tc.slim\n",
    "def depthwise_conv_bn(x, kernel_size, stride=1, dilation=1):\n",
    "    with tf.variable_scope(None, 'depthwise_conv_bn'):\n",
    "        x = slim.separable_conv2d(x, None, kernel_size, depth_multiplier=1, stride=stride,\n",
    "                                  rate=dilation, activation_fn=None, biases_initializer=None)\n",
    "        #x = slim.batch_norm(x, activation_fn=None, fused=False)\n",
    "    return x\n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 3])\n",
    "out=depthwise_conv_bn(inputs, (3,3), stride=1, dilation=1)\n",
    "print(get_num_params()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shufflenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "##參考：https://github.com/timctho/shufflenet-v2-tensorflow/blob/master/module.py\n",
    "##參考：https://github.com/TropComplique/shufflenet-v2-tensorflow/blob/master/architecture.py\n",
    "\n",
    "def shuffle_unit(x, groups):  ##一般的shuffle depthwise_conv輸出的Feature Map\n",
    "    with tf.variable_scope('shuffle_unit'):\n",
    "        n, h, w, c = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=([tf.shape(x)[0], h, w, groups, c // groups]))\n",
    "        x = tf.transpose(x, tf.convert_to_tensor([0, 1, 2, 4, 3]))\n",
    "        x = tf.reshape(x, shape=[tf.shape(x)[0], h, w, c])\n",
    "    return x\n",
    "def depthwise_conv(\n",
    "        x, kernel=3, stride=1, padding='SAME',\n",
    "        activation_fn=None, normalizer_fn=None,\n",
    "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        data_format='NHWC', scope='depthwise_conv'):      ##一般的depthwise_conv\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        assert data_format == 'NHWC'\n",
    "        in_channels = x.shape[3].value\n",
    "        W = tf.get_variable(\n",
    "            'depthwise_weights',\n",
    "            [kernel, kernel, in_channels, 1], dtype=tf.float32,\n",
    "            initializer=weights_initializer\n",
    "        )\n",
    "        x = tf.nn.depthwise_conv2d(x, W, [1, stride, stride, 1], padding, data_format='NHWC')\n",
    "        x = tf.layers.batch_normalization(x) if normalizer_fn is not None else x  # batch normalization\n",
    "        x = tf.nn.leaky_relu(x) if activation_fn is not None else x  # nonlinearity\n",
    "        return x\n",
    "    \n",
    "def conv_bn_relu(x, out_channel, kernel_size, stride=1):  ##一般的Convolution+BN+Relu\n",
    "    with tf.variable_scope(None, 'conv_bn_relu'):\n",
    "        x = tf.layers.conv2d(x, out_channel, kernel_size, stride,)\n",
    "        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x))\n",
    "    return x\n",
    "\n",
    "def shufflenet_v2_block(x, out_channel, kernel_size, stride=1, shuffle_group=2): ##shufflenet_v2_block\n",
    "    with tf.variable_scope(None, 'shuffle_v2_block'):\n",
    "        if stride == 1:\n",
    "            top, bottom = tf.split(x, num_or_size_splits=2, axis=3)\n",
    "\n",
    "            half_channel = out_channel // 2\n",
    "\n",
    "            top = conv_bn_relu(top, half_channel, 1)\n",
    "            top = depthwise_conv_bn(top, kernel_size, stride)\n",
    "            top = conv_bn_relu(top, half_channel, 1)\n",
    "\n",
    "            out = tf.concat([top, bottom], axis=3)\n",
    "            out = shuffle_unit(out, shuffle_group)\n",
    "\n",
    "        else:   ##downsampling的Block\n",
    "            half_channel = out_channel // 2\n",
    "            b0 = conv_bn_relu(x, half_channel, 1)\n",
    "            b0 = depthwise_conv_bn(b0, kernel_size, stride)\n",
    "            b0 = conv_bn_relu(b0, half_channel, 1)\n",
    "\n",
    "            b1 = depthwise_conv_bn(x, kernel_size, stride)\n",
    "            b1 = conv_bn_relu(b1, half_channel, 1)\n",
    "\n",
    "            out = tf.concat([b0, b1], axis=3)\n",
    "            out = shuffle_unit(out, shuffle_group)\n",
    "        return out\n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 4])\n",
    "out=shufflenet_v2_block(inputs, 2, (3,3), stride=1, shuffle_group=2)\n",
    "print(get_num_params()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"depthwise_conv_bn/conv2d/LeakyRelu:0\", shape=(?, 300, 300, 10), dtype=float32)\n",
      "Tensor(\"depthwise_conv_bn_1/conv2d/LeakyRelu:0\", shape=(?, 300, 300, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 6])\n",
    "with tf.variable_scope(None, 'depthwise_conv_bn'):\n",
    "    x=tf.layers.conv2d(inputs,10,(1,1),strides=(1,1),activation=tf.nn.leaky_relu)\n",
    "    print(x)\n",
    "with tf.variable_scope(None, 'depthwise_conv_bn'):\n",
    "    x=tf.layers.conv2d(x,10,(1,1),strides=(1,1),activation=tf.nn.leaky_relu)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthwise_conv(\n",
    "        x, kernel=3, stride=1, padding='SAME',\n",
    "        activation_fn=None, normalizer_fn=None,\n",
    "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        data_format='NHWC', scope='depthwise_conv'):      ##一般的depthwise_conv\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        assert data_format == 'NHWC'\n",
    "        in_channels = x.shape[3].value\n",
    "        W = tf.get_variable(\n",
    "            'depthwise_weights',\n",
    "            [kernel, kernel, in_channels, 1], dtype=tf.float32,\n",
    "            initializer=weights_initializer\n",
    "        )\n",
    "        x = tf.nn.depthwise_conv2d(x, W, [1, stride, stride, 1], padding, data_format='NHWC')\n",
    "        x = tf.layers.batch_normalization(x) if normalizer_fn is not None else x  # batch normalization\n",
    "        x = tf.nn.leaky_relu(x) if activation_fn is not None else x  # nonlinearity\n",
    "        return x\n",
    "\n",
    "\n",
    "def res_block(input, expansion_ratio, output_dim, stride, name, bias=False, shortcut=True):\n",
    "    with tf.name_scope(name), tf.variable_scope(name):\n",
    "        # pw\n",
    "        bottleneck_dim=round(expansion_ratio*input.get_shape().as_list()[-1]) \n",
    "        net = tf.layers.conv2d(input, bottleneck_dim,(1,1), name='pw', \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003),use_bias=bias) ##先擴張\n",
    "        net = tf.layers.batch_normalization(net, name='pw_bn')\n",
    "        net = tf.nn.relu6(net)\n",
    "        # dw\n",
    "        net = depthwise_conv(net)\n",
    "        net = tf.layers.batch_normalization(net, name='dw_bn')\n",
    "        net = tf.nn.relu6(net)\n",
    "        # pw & linear\n",
    "        net = tf.layers.conv2d(net, output_dim,(1,1), name='pw_linear', \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003),use_bias=bias) ##壓回輸出深度\n",
    "        net = tf.layers.batch_normalization(net,name='pw_linear_bn')\n",
    "\n",
    "        # element wise add, only for stride==1\n",
    "        if shortcut and stride == 1:\n",
    "            in_dim=int(input.get_shape().as_list()[-1])\n",
    "            if in_dim != output_dim:\n",
    "                ins=tf.layers.conv2d(input, output_dim,(1,1), name='ex_dim', \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003),use_bias=bias) \n",
    "                net=ins+net\n",
    "            else:\n",
    "                net=input+net\n",
    "\n",
    "        return net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 3])\n",
    "out=res_block(inputs,4,32,1,name='Res1')\n",
    "out=res_block(out,4,32,1,name='Res2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def depthwise_conv(\n",
    "        x, kernel=3, stride=1, padding='SAME',\n",
    "        activation_fn=None, normalizer_fn=None,\n",
    "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        data_format='NHWC', scope='depthwise_conv'):      ##一般的depthwise_conv\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        assert data_format == 'NHWC'\n",
    "        in_channels = x.shape[3].value\n",
    "        W = tf.get_variable(\n",
    "            'depthwise_weights',\n",
    "            [kernel, kernel, in_channels, 1], dtype=tf.float32,\n",
    "            initializer=weights_initializer\n",
    "        )\n",
    "        x = tf.nn.depthwise_conv2d(x, W, [1, stride, stride, 1], padding, data_format='NHWC')\n",
    "        x = tf.layers.batch_normalization(x) if normalizer_fn is not None else x  # batch normalization\n",
    "        x = tf.nn.leaky_relu(x) if activation_fn is not None else x  # nonlinearity\n",
    "        return x\n",
    "\n",
    "\n",
    "def MBConvBlock(input, expansion_ratio, output_dim, stride, name,squeeze ,bias=False, shortcut=True,use_Squeeze_Excitation=True):\n",
    "    with tf.name_scope(name), tf.variable_scope(name):\n",
    "        # pw\n",
    "        bottleneck_dim=round(expansion_ratio*input.get_shape().as_list()[-1]) \n",
    "        net = tf.layers.conv2d(input, bottleneck_dim,(1,1), name='pw', \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003),use_bias=bias) ##先擴張\n",
    "        net = tf.layers.batch_normalization(net, name='pw_bn')\n",
    "        net = tf.nn.relu6(net)\n",
    "        # dw\n",
    "        net = depthwise_conv(net,stride=stride)\n",
    "        net = tf.layers.batch_normalization(net, name='dw_bn')\n",
    "        net = tf.nn.relu6(net)\n",
    "        # pw & linear\n",
    "        net = tf.layers.conv2d(net, output_dim,(1,1),name='pw_linear', \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003),use_bias=bias) ##壓回輸出深度\n",
    "        net = tf.layers.batch_normalization(net,name='pw_linear_bn')\n",
    "        \n",
    "        if use_Squeeze_Excitation:\n",
    "            in_dim=int(net.get_shape().as_list()[-1])\n",
    "            Squeeze=tf.layers.average_pooling2d(net, net.get_shape()[1:-1], 1)\n",
    "            Squeeze=tf.nn.relu(tf.layers.dense(Squeeze, use_bias=False, units=in_dim//squeeze))\n",
    "            Excitation=tf.nn.relu(tf.layers.dense(Squeeze, use_bias=False, units=output_dim))\n",
    "            Excitation=tf.nn.sigmoid(Excitation)\n",
    "            net = tf.reshape(Excitation, [-1,1,1,output_dim])*net\n",
    "\n",
    "        # SENET-Squeeze-Excitation\n",
    "        in_dim=int(input.get_shape().as_list()[-1])\n",
    "        if shortcut and stride == 1:\n",
    "            if in_dim != output_dim:\n",
    "                ins=tf.layers.conv2d(input, output_dim,(1,1), name='ex_dim', \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003),use_bias=bias) \n",
    "                net=ins+net\n",
    "            else:\n",
    "                net=input+net\n",
    "\n",
    "        return net\n",
    "    \n",
    "    \n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 43])\n",
    "out=MBConvBlock(inputs, 4, 64, 1, 'first', 4,bias=False, shortcut=True,use_Squeeze_Excitation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 43])\n",
    "out=MBConvBlock(inputs, 4, 64, 1, 'first', 4,bias=False, shortcut=True,use_Squeeze_Excitation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobilenetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.nn.relu6(-4 + 3.) / 6.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def Hswish(input_):\n",
    "    return input_* tf.nn.relu6(input_ + 3.) / 6.\n",
    "\n",
    "def Hsigmoid(input_):\n",
    "    return tf.nn.relu6(input_ + 3.) / 6.\n",
    "\n",
    "def depthwise_conv(\n",
    "        x, kernel=3, stride=1, padding='SAME',\n",
    "        activation_fn=None, normalizer_fn=None,\n",
    "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        data_format='NHWC', scope='depthwise_conv'):      ##一般的depthwise_conv\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        assert data_format == 'NHWC'\n",
    "        in_channels = x.shape[3].value\n",
    "        W = tf.get_variable(\n",
    "            'depthwise_weights',\n",
    "            [kernel, kernel, in_channels, 1], dtype=tf.float32,\n",
    "            initializer=weights_initializer\n",
    "        )\n",
    "        x = tf.nn.depthwise_conv2d(x, W, [1, stride, stride, 1], padding, data_format='NHWC',)\n",
    "        x = tf.layers.batch_normalization(x) if normalizer_fn is not None else x  # batch normalization\n",
    "        x = tf.nn.leaky_relu(x) if activation_fn is not None else x  # nonlinearity\n",
    "        return x\n",
    "    \n",
    "def SEBlock(input_,squeeze=4):\n",
    "    in_dim=int(input_.get_shape().as_list()[-1])\n",
    "    print( input_.get_shape()[1:-1])\n",
    "    Squeeze=tf.layers.average_pooling2d(input_, input_.get_shape()[1:-1], 1)\n",
    "    Squeeze=tf.nn.relu(tf.layers.dense(Squeeze, use_bias=False, units=in_dim//squeeze)) \n",
    "    Excitation=tf.nn.relu(tf.layers.dense(Squeeze, use_bias=False, units=in_dim))\n",
    "    Excitation=Hsigmoid(Excitation) ##Hsigmoid replace Sigmoid\n",
    "    Excitation = tf.reshape(Excitation, [-1,1,1,in_dim])\n",
    "    return input_*Excitation\n",
    "    \n",
    "    \n",
    "def MobileV3Bottleneck(input_,expand_size,squeeze,out_size,kernel_size,stride=1,relu=True,se=True):\n",
    "    Shortcut=input_\n",
    "    in_dim=int(input_.get_shape().as_list()[-1])\n",
    "    out=tf.layers.batch_normalization(tf.layers.conv2d(input_,expand_size,(1,1),(1,1),use_bias=False))\n",
    "    if relu:\n",
    "        out=tf.nn.relu(out) #or relu6\n",
    "    else:\n",
    "        out=Hswish(out)\n",
    "        \n",
    "    \n",
    "    out=depthwise_conv(out,kernel=kernel_size,stride=stride, padding='SAME')\n",
    "    out=tf.layers.batch_normalization(out)\n",
    "    if relu:\n",
    "        out=tf.nn.relu(out) #or relu6\n",
    "    else:\n",
    "        out=Hswish(out)\n",
    "        \n",
    "    out=tf.layers.batch_normalization(tf.layers.conv2d(input_,out_size,(1,1),(1,1),use_bias=False))\n",
    "    \n",
    "    if (in_dim != out_size) and (stride == 1):\n",
    "        Shortcut=tf.layers.conv2d(Shortcut,out_size, (1,1), strides = (stride, stride),use_bias=False)\n",
    "        Shortcut = tf.layers.batch_normalization(Shortcut)\n",
    "    if se:\n",
    "        assert squeeze<=out_size\n",
    "        out=SEBlock(out,squeeze=squeeze)\n",
    "\n",
    "    out = out +Shortcut if stride==1 else out\n",
    "    return out\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, [None, 300, 300, 80])\n",
    "out=MobileV3Bottleneck(inputs,480,4,112,3,stride=1,relu=False,se=True)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
